{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "919eda0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7e5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"meta_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e3f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_rows</th>\n",
       "      <th>sales_mean</th>\n",
       "      <th>sales_std</th>\n",
       "      <th>sales_cv</th>\n",
       "      <th>sales_skew</th>\n",
       "      <th>sales_kurtosis</th>\n",
       "      <th>closed_fraction</th>\n",
       "      <th>weekend_ratio</th>\n",
       "      <th>customers_mean</th>\n",
       "      <th>customers_std</th>\n",
       "      <th>pca_var_first</th>\n",
       "      <th>pca_components_90</th>\n",
       "      <th>store_type</th>\n",
       "      <th>assortment</th>\n",
       "      <th>competition_mean</th>\n",
       "      <th>competition_std</th>\n",
       "      <th>label</th>\n",
       "      <th>leaf_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>743</td>\n",
       "      <td>4593.7140</td>\n",
       "      <td>2435.7651</td>\n",
       "      <td>0.530239</td>\n",
       "      <td>-0.506928</td>\n",
       "      <td>-0.041492</td>\n",
       "      <td>0.160162</td>\n",
       "      <td>0.829754</td>\n",
       "      <td>537.504711</td>\n",
       "      <td>265.109817</td>\n",
       "      <td>0.997754</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>741</td>\n",
       "      <td>5661.2210</td>\n",
       "      <td>2928.6194</td>\n",
       "      <td>0.517312</td>\n",
       "      <td>-0.786354</td>\n",
       "      <td>0.092722</td>\n",
       "      <td>0.172740</td>\n",
       "      <td>0.939018</td>\n",
       "      <td>514.367072</td>\n",
       "      <td>258.919846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8090.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>754</td>\n",
       "      <td>5204.5977</td>\n",
       "      <td>2756.3560</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>-0.568414</td>\n",
       "      <td>-0.033909</td>\n",
       "      <td>0.157825</td>\n",
       "      <td>0.800631</td>\n",
       "      <td>691.066313</td>\n",
       "      <td>344.848645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_331</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>761</td>\n",
       "      <td>5704.5786</td>\n",
       "      <td>2961.0244</td>\n",
       "      <td>0.519061</td>\n",
       "      <td>-0.701369</td>\n",
       "      <td>0.099789</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>1.059587</td>\n",
       "      <td>474.249671</td>\n",
       "      <td>231.376864</td>\n",
       "      <td>0.997550</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9230.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_572</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>760</td>\n",
       "      <td>10132.8430</td>\n",
       "      <td>6128.6270</td>\n",
       "      <td>0.604828</td>\n",
       "      <td>-0.221403</td>\n",
       "      <td>-0.675996</td>\n",
       "      <td>0.171053</td>\n",
       "      <td>0.553850</td>\n",
       "      <td>1120.869565</td>\n",
       "      <td>638.839728</td>\n",
       "      <td>0.996584</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_1014</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>743</td>\n",
       "      <td>6398.4560</td>\n",
       "      <td>3802.4944</td>\n",
       "      <td>0.594283</td>\n",
       "      <td>-0.231925</td>\n",
       "      <td>-0.632953</td>\n",
       "      <td>0.164199</td>\n",
       "      <td>0.543334</td>\n",
       "      <td>655.640646</td>\n",
       "      <td>365.431059</td>\n",
       "      <td>0.998150</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19370.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_892</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>749</td>\n",
       "      <td>5404.1533</td>\n",
       "      <td>3000.7166</td>\n",
       "      <td>0.555261</td>\n",
       "      <td>-0.424574</td>\n",
       "      <td>-0.241628</td>\n",
       "      <td>0.172230</td>\n",
       "      <td>1.237494</td>\n",
       "      <td>465.543391</td>\n",
       "      <td>239.973048</td>\n",
       "      <td>0.996873</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>970.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_785</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>765</td>\n",
       "      <td>7827.6973</td>\n",
       "      <td>3453.6255</td>\n",
       "      <td>0.441206</td>\n",
       "      <td>-0.537519</td>\n",
       "      <td>0.061153</td>\n",
       "      <td>0.048366</td>\n",
       "      <td>0.977242</td>\n",
       "      <td>666.751634</td>\n",
       "      <td>264.603756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_310</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>772</td>\n",
       "      <td>6629.6490</td>\n",
       "      <td>3733.9258</td>\n",
       "      <td>0.563216</td>\n",
       "      <td>-0.706442</td>\n",
       "      <td>-0.315641</td>\n",
       "      <td>0.202073</td>\n",
       "      <td>1.164527</td>\n",
       "      <td>779.966321</td>\n",
       "      <td>419.274426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_699</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>751</td>\n",
       "      <td>5549.1216</td>\n",
       "      <td>2844.7588</td>\n",
       "      <td>0.512650</td>\n",
       "      <td>-0.789140</td>\n",
       "      <td>0.082043</td>\n",
       "      <td>0.162450</td>\n",
       "      <td>1.230442</td>\n",
       "      <td>749.230360</td>\n",
       "      <td>359.185399</td>\n",
       "      <td>0.995955</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_1044</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1110 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_rows  sales_mean  sales_std  sales_cv  sales_skew  sales_kurtosis  \\\n",
       "0          743   4593.7140  2435.7651  0.530239   -0.506928       -0.041492   \n",
       "1          741   5661.2210  2928.6194  0.517312   -0.786354        0.092722   \n",
       "2          754   5204.5977  2756.3560  0.529600   -0.568414       -0.033909   \n",
       "3          761   5704.5786  2961.0244  0.519061   -0.701369        0.099789   \n",
       "4          760  10132.8430  6128.6270  0.604828   -0.221403       -0.675996   \n",
       "...        ...         ...        ...       ...         ...             ...   \n",
       "1105       743   6398.4560  3802.4944  0.594283   -0.231925       -0.632953   \n",
       "1106       749   5404.1533  3000.7166  0.555261   -0.424574       -0.241628   \n",
       "1107       765   7827.6973  3453.6255  0.441206   -0.537519        0.061153   \n",
       "1108       772   6629.6490  3733.9258  0.563216   -0.706442       -0.315641   \n",
       "1109       751   5549.1216  2844.7588  0.512650   -0.789140        0.082043   \n",
       "\n",
       "      closed_fraction  weekend_ratio  customers_mean  customers_std  \\\n",
       "0            0.160162       0.829754      537.504711     265.109817   \n",
       "1            0.172740       0.939018      514.367072     258.919846   \n",
       "2            0.157825       0.800631      691.066313     344.848645   \n",
       "3            0.168200       1.059587      474.249671     231.376864   \n",
       "4            0.171053       0.553850     1120.869565     638.839728   \n",
       "...               ...            ...             ...            ...   \n",
       "1105         0.164199       0.543334      655.640646     365.431059   \n",
       "1106         0.172230       1.237494      465.543391     239.973048   \n",
       "1107         0.048366       0.977242      666.751634     264.603756   \n",
       "1108         0.202073       1.164527      779.966321     419.274426   \n",
       "1109         0.162450       1.230442      749.230360     359.185399   \n",
       "\n",
       "      pca_var_first  pca_components_90  store_type  assortment  \\\n",
       "0          0.997754                  1           0           0   \n",
       "1          0.000000                 18           0           2   \n",
       "2          0.000000                 18           0           2   \n",
       "3          0.997550                  1           3           2   \n",
       "4          0.996584                  1           0           2   \n",
       "...             ...                ...         ...         ...   \n",
       "1105       0.998150                  1           0           0   \n",
       "1106       0.996873                  1           3           2   \n",
       "1107       0.000000                 18           0           2   \n",
       "1108       0.000000                 18           0           0   \n",
       "1109       0.995955                  1           2           0   \n",
       "\n",
       "      competition_mean  competition_std       label  leaf_id  \n",
       "0                540.0              0.0    store_44        0  \n",
       "1               8090.0              0.0   store_346        1  \n",
       "2                670.0              0.0   store_331        2  \n",
       "3               9230.0              0.0   store_572        1  \n",
       "4                210.0              0.0  store_1014        3  \n",
       "...                ...              ...         ...      ...  \n",
       "1105           19370.0              0.0   store_892        4  \n",
       "1106             970.0              0.0   store_785        1  \n",
       "1107            2290.0              0.0   store_310        4  \n",
       "1108             180.0              0.0   store_699        6  \n",
       "1109             240.0              0.0  store_1044        6  \n",
       "\n",
       "[1110 rows x 18 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df.iloc[0:1110]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "baeab9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"sales_mean\"\n",
    "cart_features = [\n",
    "    \"num_rows\",\n",
    "    \"customers_mean\",\n",
    "    \"customers_std\",\n",
    "    \"pca_var_first\",\n",
    "    \"store_type\",\n",
    "    \"assortment\",\n",
    "]\n",
    "\n",
    "cart_df = df_train[cart_features + [TARGET]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a60aab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(rows):\n",
    "    return np.var(rows[:, -1])\n",
    "\n",
    "def variance_reduction(left, right, current_var):\n",
    "    n = len(left) + len(right)\n",
    "    return current_var - (\n",
    "        len(left)/n * variance(left) + len(right)/n * variance(right)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17328178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    def __init__(self, column, value, feature_names):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "    def match(self, row):\n",
    "        return row[self.column] >= self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Is {self.feature_names[self.column]} >= {self.value}?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55800c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(rows, question):\n",
    "    mask = rows[:, question.column] >= question.value\n",
    "    return rows[mask], rows[~mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc0b7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(rows, feature_names):\n",
    "    best_gain = 0\n",
    "    best_question = None\n",
    "    current_var = variance(rows)\n",
    "    n_features = rows.shape[1] - 1\n",
    "\n",
    "    for col in range(n_features):\n",
    "        values = np.unique(rows[:, col])\n",
    "        for val in values:\n",
    "            q = Question(col, val, feature_names)\n",
    "            left, right = partition(rows, q)\n",
    "            if len(left) == 0 or len(right) == 0:\n",
    "                continue\n",
    "            gain = variance_reduction(left, right, current_var)\n",
    "            if gain > best_gain:\n",
    "                best_gain, best_question = gain, q\n",
    "\n",
    "    return best_gain, best_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea703a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    def __init__(self, rows):\n",
    "        self.value = rows[:, -1].mean()\n",
    "        self.n_samples = len(rows)\n",
    "\n",
    "class DecisionNode:\n",
    "    def __init__(self, question, left, right):\n",
    "        self.question = question\n",
    "        self.left = left\n",
    "        self.right = right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e95e7875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeafCounter:\n",
    "    def __init__(self, max_leaves):\n",
    "        self.max_leaves = max_leaves\n",
    "        self.count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99cf4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(rows, feature_names, counter, min_samples_leaf=1000):\n",
    "    gain, question = find_best_split(rows, feature_names)\n",
    "\n",
    "    if gain == 0 or len(rows) <= min_samples_leaf or counter.count >= counter.max_leaves:\n",
    "        counter.count += 1\n",
    "        return Leaf(rows)\n",
    "\n",
    "    left, right = partition(rows, question)\n",
    "\n",
    "    if counter.count + 2 > counter.max_leaves:\n",
    "        counter.count += 1\n",
    "        return Leaf(rows)\n",
    "\n",
    "    left_branch = build_tree(left, feature_names, counter, min_samples_leaf)\n",
    "    right_branch = build_tree(right, feature_names, counter, min_samples_leaf)\n",
    "\n",
    "    return DecisionNode(question, left_branch, right_branch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e351b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_array = cart_df.to_numpy()\n",
    "counter = LeafCounter(max_leaves=10)\n",
    "\n",
    "cart_tree = build_tree(\n",
    "    cart_array,\n",
    "    cart_features,\n",
    "    counter,\n",
    "    min_samples_leaf=100  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ae4d786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is customers_mean >= 823.8724226804123?\n",
      "--> True:\n",
      "  Is customers_mean >= 1357.2453333333333?\n",
      "  --> True:\n",
      "    Leaf: value=12654.71, samples=30\n",
      "  --> False:\n",
      "    Is customers_mean >= 1044.448700410397?\n",
      "    --> True:\n",
      "      Leaf: value=9180.52, samples=41\n",
      "    --> False:\n",
      "      Leaf: value=7545.39, samples=100\n",
      "--> False:\n",
      "  Is customers_mean >= 533.8319738988581?\n",
      "  --> True:\n",
      "    Is store_type >= 3.0?\n",
      "    --> True:\n",
      "      Is customers_mean >= 654.5797101449275?\n",
      "      --> True:\n",
      "        Leaf: value=7608.62, samples=34\n",
      "      --> False:\n",
      "        Leaf: value=6386.37, samples=86\n",
      "    --> False:\n",
      "      Is customers_mean >= 672.8653594771242?\n",
      "      --> True:\n",
      "        Is pca_var_first >= 0.9979759721210184?\n",
      "        --> True:\n",
      "          Leaf: value=7005.87, samples=19\n",
      "        --> False:\n",
      "          Is assortment >= 2.0?\n",
      "          --> True:\n",
      "            Leaf: value=6410.56, samples=51\n",
      "          --> False:\n",
      "            Leaf: value=6046.64, samples=69\n",
      "      --> False:\n",
      "        Is customers_mean >= 610.6013071895425?\n",
      "        --> True:\n",
      "          Leaf: value=5743.33, samples=74\n",
      "        --> False:\n",
      "          Leaf: value=5226.71, samples=114\n",
      "  --> False:\n",
      "    Leaf: value=4435.96, samples=492\n"
     ]
    }
   ],
   "source": [
    "def print_tree(node, spacing=\"\"):\n",
    "    if isinstance(node, Leaf):\n",
    "        print(spacing + f\"Leaf: value={node.value:.2f}, samples={node.n_samples}\")\n",
    "        return\n",
    "\n",
    "    print(spacing + str(node.question))\n",
    "    print(spacing + \"--> True:\")\n",
    "    print_tree(node.left, spacing + \"  \")\n",
    "    print(spacing + \"--> False:\")\n",
    "    print_tree(node.right, spacing + \"  \")\n",
    "\n",
    "print_tree(cart_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9cc2c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_id_map = {}\n",
    "\n",
    "def get_leaf_id(node, row):\n",
    "    if isinstance(node, Leaf):\n",
    "        if node not in leaf_id_map:\n",
    "            leaf_id_map[node] = len(leaf_id_map)  # 0,1,2,3,4\n",
    "        return leaf_id_map[node]\n",
    "    if node.question.match(row):\n",
    "        return get_leaf_id(node.left, row)\n",
    "    else:\n",
    "        return get_leaf_id(node.right, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8820694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m_/m50wq84x1gv7lmgz099xvzd80000gn/T/ipykernel_48888/3706174785.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"leaf_id\"] = [\n"
     ]
    }
   ],
   "source": [
    "df_train[\"leaf_id\"] = [\n",
    "    get_leaf_id(cart_tree, row)\n",
    "    for row in cart_array\n",
    "]\n",
    "\n",
    "print(df_train[\"leaf_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2fe6add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_rows</th>\n",
       "      <th>sales_mean</th>\n",
       "      <th>sales_std</th>\n",
       "      <th>sales_cv</th>\n",
       "      <th>sales_skew</th>\n",
       "      <th>sales_kurtosis</th>\n",
       "      <th>closed_fraction</th>\n",
       "      <th>weekend_ratio</th>\n",
       "      <th>customers_mean</th>\n",
       "      <th>customers_std</th>\n",
       "      <th>pca_var_first</th>\n",
       "      <th>pca_components_90</th>\n",
       "      <th>store_type</th>\n",
       "      <th>assortment</th>\n",
       "      <th>competition_mean</th>\n",
       "      <th>competition_std</th>\n",
       "      <th>label</th>\n",
       "      <th>leaf_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>743</td>\n",
       "      <td>4593.7140</td>\n",
       "      <td>2435.7651</td>\n",
       "      <td>0.530239</td>\n",
       "      <td>-0.506928</td>\n",
       "      <td>-0.041492</td>\n",
       "      <td>0.160162</td>\n",
       "      <td>0.829754</td>\n",
       "      <td>537.504711</td>\n",
       "      <td>265.109817</td>\n",
       "      <td>0.997754</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>760</td>\n",
       "      <td>4617.4546</td>\n",
       "      <td>2419.8123</td>\n",
       "      <td>0.524058</td>\n",
       "      <td>-0.525530</td>\n",
       "      <td>0.149822</td>\n",
       "      <td>0.155263</td>\n",
       "      <td>1.028929</td>\n",
       "      <td>534.827632</td>\n",
       "      <td>254.053573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_1096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>747</td>\n",
       "      <td>4553.5690</td>\n",
       "      <td>2397.7578</td>\n",
       "      <td>0.526567</td>\n",
       "      <td>-0.670128</td>\n",
       "      <td>0.312879</td>\n",
       "      <td>0.171352</td>\n",
       "      <td>1.187217</td>\n",
       "      <td>580.425703</td>\n",
       "      <td>289.974471</td>\n",
       "      <td>0.996778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>610</td>\n",
       "      <td>5257.4067</td>\n",
       "      <td>2655.3198</td>\n",
       "      <td>0.505063</td>\n",
       "      <td>-1.114039</td>\n",
       "      <td>0.021207</td>\n",
       "      <td>0.181967</td>\n",
       "      <td>1.314789</td>\n",
       "      <td>544.652459</td>\n",
       "      <td>267.965988</td>\n",
       "      <td>0.997654</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_650</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>757</td>\n",
       "      <td>6346.3145</td>\n",
       "      <td>4069.1433</td>\n",
       "      <td>0.641182</td>\n",
       "      <td>0.111423</td>\n",
       "      <td>-0.501225</td>\n",
       "      <td>0.180978</td>\n",
       "      <td>0.683672</td>\n",
       "      <td>566.253633</td>\n",
       "      <td>315.540259</td>\n",
       "      <td>0.998026</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_552</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>736</td>\n",
       "      <td>5590.7847</td>\n",
       "      <td>3073.3752</td>\n",
       "      <td>0.549722</td>\n",
       "      <td>-0.553525</td>\n",
       "      <td>-0.217831</td>\n",
       "      <td>0.177989</td>\n",
       "      <td>0.870962</td>\n",
       "      <td>560.369565</td>\n",
       "      <td>287.880293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5460.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>737</td>\n",
       "      <td>5786.1406</td>\n",
       "      <td>3580.9856</td>\n",
       "      <td>0.618890</td>\n",
       "      <td>-0.173092</td>\n",
       "      <td>-0.808705</td>\n",
       "      <td>0.170963</td>\n",
       "      <td>0.547431</td>\n",
       "      <td>594.278155</td>\n",
       "      <td>335.908267</td>\n",
       "      <td>0.997656</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_156</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>756</td>\n",
       "      <td>4812.8270</td>\n",
       "      <td>3019.6497</td>\n",
       "      <td>0.627417</td>\n",
       "      <td>-0.205477</td>\n",
       "      <td>-0.784308</td>\n",
       "      <td>0.179894</td>\n",
       "      <td>0.517286</td>\n",
       "      <td>579.829365</td>\n",
       "      <td>341.142807</td>\n",
       "      <td>0.998239</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>611</td>\n",
       "      <td>5271.2964</td>\n",
       "      <td>3210.9730</td>\n",
       "      <td>0.609143</td>\n",
       "      <td>-0.200020</td>\n",
       "      <td>-0.441517</td>\n",
       "      <td>0.183306</td>\n",
       "      <td>0.687835</td>\n",
       "      <td>547.859247</td>\n",
       "      <td>302.365331</td>\n",
       "      <td>0.997829</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12610.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>599</td>\n",
       "      <td>6123.3150</td>\n",
       "      <td>3087.7961</td>\n",
       "      <td>0.504269</td>\n",
       "      <td>-1.084589</td>\n",
       "      <td>0.139010</td>\n",
       "      <td>0.175292</td>\n",
       "      <td>1.236997</td>\n",
       "      <td>560.744574</td>\n",
       "      <td>273.386626</td>\n",
       "      <td>0.996792</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2230.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_rows  sales_mean  sales_std  sales_cv  sales_skew  sales_kurtosis  \\\n",
       "0          743   4593.7140  2435.7651  0.530239   -0.506928       -0.041492   \n",
       "8          760   4617.4546  2419.8123  0.524058   -0.525530        0.149822   \n",
       "11         747   4553.5690  2397.7578  0.526567   -0.670128        0.312879   \n",
       "30         610   5257.4067  2655.3198  0.505063   -1.114039        0.021207   \n",
       "33         757   6346.3145  4069.1433  0.641182    0.111423       -0.501225   \n",
       "...        ...         ...        ...       ...         ...             ...   \n",
       "1085       736   5590.7847  3073.3752  0.549722   -0.553525       -0.217831   \n",
       "1087       737   5786.1406  3580.9856  0.618890   -0.173092       -0.808705   \n",
       "1091       756   4812.8270  3019.6497  0.627417   -0.205477       -0.784308   \n",
       "1097       611   5271.2964  3210.9730  0.609143   -0.200020       -0.441517   \n",
       "1102       599   6123.3150  3087.7961  0.504269   -1.084589        0.139010   \n",
       "\n",
       "      closed_fraction  weekend_ratio  customers_mean  customers_std  \\\n",
       "0            0.160162       0.829754      537.504711     265.109817   \n",
       "8            0.155263       1.028929      534.827632     254.053573   \n",
       "11           0.171352       1.187217      580.425703     289.974471   \n",
       "30           0.181967       1.314789      544.652459     267.965988   \n",
       "33           0.180978       0.683672      566.253633     315.540259   \n",
       "...               ...            ...             ...            ...   \n",
       "1085         0.177989       0.870962      560.369565     287.880293   \n",
       "1087         0.170963       0.547431      594.278155     335.908267   \n",
       "1091         0.179894       0.517286      579.829365     341.142807   \n",
       "1097         0.183306       0.687835      547.859247     302.365331   \n",
       "1102         0.175292       1.236997      560.744574     273.386626   \n",
       "\n",
       "      pca_var_first  pca_components_90  store_type  assortment  \\\n",
       "0          0.997754                  1           0           0   \n",
       "8          0.000000                 18           0           2   \n",
       "11         0.996778                  1           0           0   \n",
       "30         0.997654                  1           0           0   \n",
       "33         0.998026                  1           0           0   \n",
       "...             ...                ...         ...         ...   \n",
       "1085       0.000000                 18           0           2   \n",
       "1087       0.997656                  1           0           0   \n",
       "1091       0.998239                  1           0           0   \n",
       "1097       0.997829                  1           0           0   \n",
       "1102       0.996792                  1           0           2   \n",
       "\n",
       "      competition_mean  competition_std       label  leaf_id  \n",
       "0                540.0              0.0    store_44        0  \n",
       "8               1130.0              0.0  store_1096        0  \n",
       "11               700.0              0.0   store_563        0  \n",
       "30              1420.0              0.0   store_650        0  \n",
       "33              4260.0              0.0   store_552        0  \n",
       "...                ...              ...         ...      ...  \n",
       "1085            5460.0              0.0   store_934        0  \n",
       "1087            2020.0              0.0   store_156        0  \n",
       "1091            2720.0              0.0   store_836        0  \n",
       "1097           12610.0              0.0   store_181        0  \n",
       "1102            2230.0              0.0  store_1000        0  \n",
       "\n",
       "[114 rows x 18 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train[\"leaf_id\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "273ee37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_rows</th>\n",
       "      <th>sales_mean</th>\n",
       "      <th>sales_std</th>\n",
       "      <th>sales_cv</th>\n",
       "      <th>sales_skew</th>\n",
       "      <th>sales_kurtosis</th>\n",
       "      <th>closed_fraction</th>\n",
       "      <th>weekend_ratio</th>\n",
       "      <th>customers_mean</th>\n",
       "      <th>customers_std</th>\n",
       "      <th>pca_var_first</th>\n",
       "      <th>pca_components_90</th>\n",
       "      <th>store_type</th>\n",
       "      <th>assortment</th>\n",
       "      <th>competition_mean</th>\n",
       "      <th>competition_std</th>\n",
       "      <th>label</th>\n",
       "      <th>leaf_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>743</td>\n",
       "      <td>4593.7140</td>\n",
       "      <td>2435.7651</td>\n",
       "      <td>0.530239</td>\n",
       "      <td>-0.506928</td>\n",
       "      <td>-0.041492</td>\n",
       "      <td>0.160162</td>\n",
       "      <td>0.829754</td>\n",
       "      <td>537.504711</td>\n",
       "      <td>265.109817</td>\n",
       "      <td>0.997754</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>741</td>\n",
       "      <td>5661.2210</td>\n",
       "      <td>2928.6194</td>\n",
       "      <td>0.517312</td>\n",
       "      <td>-0.786354</td>\n",
       "      <td>0.092722</td>\n",
       "      <td>0.172740</td>\n",
       "      <td>0.939018</td>\n",
       "      <td>514.367072</td>\n",
       "      <td>258.919846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8090.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>754</td>\n",
       "      <td>5204.5977</td>\n",
       "      <td>2756.3560</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>-0.568414</td>\n",
       "      <td>-0.033909</td>\n",
       "      <td>0.157825</td>\n",
       "      <td>0.800631</td>\n",
       "      <td>691.066313</td>\n",
       "      <td>344.848645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_331</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>761</td>\n",
       "      <td>5704.5786</td>\n",
       "      <td>2961.0244</td>\n",
       "      <td>0.519061</td>\n",
       "      <td>-0.701369</td>\n",
       "      <td>0.099789</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>1.059587</td>\n",
       "      <td>474.249671</td>\n",
       "      <td>231.376864</td>\n",
       "      <td>0.997550</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9230.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_572</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>760</td>\n",
       "      <td>10132.8430</td>\n",
       "      <td>6128.6270</td>\n",
       "      <td>0.604828</td>\n",
       "      <td>-0.221403</td>\n",
       "      <td>-0.675996</td>\n",
       "      <td>0.171053</td>\n",
       "      <td>0.553850</td>\n",
       "      <td>1120.869565</td>\n",
       "      <td>638.839728</td>\n",
       "      <td>0.996584</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_1014</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>743</td>\n",
       "      <td>6398.4560</td>\n",
       "      <td>3802.4944</td>\n",
       "      <td>0.594283</td>\n",
       "      <td>-0.231925</td>\n",
       "      <td>-0.632953</td>\n",
       "      <td>0.164199</td>\n",
       "      <td>0.543334</td>\n",
       "      <td>655.640646</td>\n",
       "      <td>365.431059</td>\n",
       "      <td>0.998150</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19370.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_892</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>749</td>\n",
       "      <td>5404.1533</td>\n",
       "      <td>3000.7166</td>\n",
       "      <td>0.555261</td>\n",
       "      <td>-0.424574</td>\n",
       "      <td>-0.241628</td>\n",
       "      <td>0.172230</td>\n",
       "      <td>1.237494</td>\n",
       "      <td>465.543391</td>\n",
       "      <td>239.973048</td>\n",
       "      <td>0.996873</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>970.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_785</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>765</td>\n",
       "      <td>7827.6973</td>\n",
       "      <td>3453.6255</td>\n",
       "      <td>0.441206</td>\n",
       "      <td>-0.537519</td>\n",
       "      <td>0.061153</td>\n",
       "      <td>0.048366</td>\n",
       "      <td>0.977242</td>\n",
       "      <td>666.751634</td>\n",
       "      <td>264.603756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_310</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>772</td>\n",
       "      <td>6629.6490</td>\n",
       "      <td>3733.9258</td>\n",
       "      <td>0.563216</td>\n",
       "      <td>-0.706442</td>\n",
       "      <td>-0.315641</td>\n",
       "      <td>0.202073</td>\n",
       "      <td>1.164527</td>\n",
       "      <td>779.966321</td>\n",
       "      <td>419.274426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_699</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>751</td>\n",
       "      <td>5549.1216</td>\n",
       "      <td>2844.7588</td>\n",
       "      <td>0.512650</td>\n",
       "      <td>-0.789140</td>\n",
       "      <td>0.082043</td>\n",
       "      <td>0.162450</td>\n",
       "      <td>1.230442</td>\n",
       "      <td>749.230360</td>\n",
       "      <td>359.185399</td>\n",
       "      <td>0.995955</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_1044</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1110 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_rows  sales_mean  sales_std  sales_cv  sales_skew  sales_kurtosis  \\\n",
       "0          743   4593.7140  2435.7651  0.530239   -0.506928       -0.041492   \n",
       "1          741   5661.2210  2928.6194  0.517312   -0.786354        0.092722   \n",
       "2          754   5204.5977  2756.3560  0.529600   -0.568414       -0.033909   \n",
       "3          761   5704.5786  2961.0244  0.519061   -0.701369        0.099789   \n",
       "4          760  10132.8430  6128.6270  0.604828   -0.221403       -0.675996   \n",
       "...        ...         ...        ...       ...         ...             ...   \n",
       "1105       743   6398.4560  3802.4944  0.594283   -0.231925       -0.632953   \n",
       "1106       749   5404.1533  3000.7166  0.555261   -0.424574       -0.241628   \n",
       "1107       765   7827.6973  3453.6255  0.441206   -0.537519        0.061153   \n",
       "1108       772   6629.6490  3733.9258  0.563216   -0.706442       -0.315641   \n",
       "1109       751   5549.1216  2844.7588  0.512650   -0.789140        0.082043   \n",
       "\n",
       "      closed_fraction  weekend_ratio  customers_mean  customers_std  \\\n",
       "0            0.160162       0.829754      537.504711     265.109817   \n",
       "1            0.172740       0.939018      514.367072     258.919846   \n",
       "2            0.157825       0.800631      691.066313     344.848645   \n",
       "3            0.168200       1.059587      474.249671     231.376864   \n",
       "4            0.171053       0.553850     1120.869565     638.839728   \n",
       "...               ...            ...             ...            ...   \n",
       "1105         0.164199       0.543334      655.640646     365.431059   \n",
       "1106         0.172230       1.237494      465.543391     239.973048   \n",
       "1107         0.048366       0.977242      666.751634     264.603756   \n",
       "1108         0.202073       1.164527      779.966321     419.274426   \n",
       "1109         0.162450       1.230442      749.230360     359.185399   \n",
       "\n",
       "      pca_var_first  pca_components_90  store_type  assortment  \\\n",
       "0          0.997754                  1           0           0   \n",
       "1          0.000000                 18           0           2   \n",
       "2          0.000000                 18           0           2   \n",
       "3          0.997550                  1           3           2   \n",
       "4          0.996584                  1           0           2   \n",
       "...             ...                ...         ...         ...   \n",
       "1105       0.998150                  1           0           0   \n",
       "1106       0.996873                  1           3           2   \n",
       "1107       0.000000                 18           0           2   \n",
       "1108       0.000000                 18           0           0   \n",
       "1109       0.995955                  1           2           0   \n",
       "\n",
       "      competition_mean  competition_std       label  leaf_id  \n",
       "0                540.0              0.0    store_44        0  \n",
       "1               8090.0              0.0   store_346        1  \n",
       "2                670.0              0.0   store_331        2  \n",
       "3               9230.0              0.0   store_572        1  \n",
       "4                210.0              0.0  store_1014        3  \n",
       "...                ...              ...         ...      ...  \n",
       "1105           19370.0              0.0   store_892        4  \n",
       "1106             970.0              0.0   store_785        1  \n",
       "1107            2290.0              0.0   store_310        4  \n",
       "1108             180.0              0.0   store_699        6  \n",
       "1109             240.0              0.0  store_1044        6  \n",
       "\n",
       "[1110 rows x 18 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "044ed4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m_/m50wq84x1gv7lmgz099xvzd80000gn/T/ipykernel_48888/3363537529.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"label\"] = df_train[\"label\"].str.replace(\"store_\", \"\", regex=False).astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_rows</th>\n",
       "      <th>sales_mean</th>\n",
       "      <th>sales_std</th>\n",
       "      <th>sales_cv</th>\n",
       "      <th>sales_skew</th>\n",
       "      <th>sales_kurtosis</th>\n",
       "      <th>closed_fraction</th>\n",
       "      <th>weekend_ratio</th>\n",
       "      <th>customers_mean</th>\n",
       "      <th>customers_std</th>\n",
       "      <th>pca_var_first</th>\n",
       "      <th>pca_components_90</th>\n",
       "      <th>store_type</th>\n",
       "      <th>assortment</th>\n",
       "      <th>competition_mean</th>\n",
       "      <th>competition_std</th>\n",
       "      <th>label</th>\n",
       "      <th>leaf_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>743</td>\n",
       "      <td>4593.7140</td>\n",
       "      <td>2435.7651</td>\n",
       "      <td>0.530239</td>\n",
       "      <td>-0.506928</td>\n",
       "      <td>-0.041492</td>\n",
       "      <td>0.160162</td>\n",
       "      <td>0.829754</td>\n",
       "      <td>537.504711</td>\n",
       "      <td>265.109817</td>\n",
       "      <td>0.997754</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>741</td>\n",
       "      <td>5661.2210</td>\n",
       "      <td>2928.6194</td>\n",
       "      <td>0.517312</td>\n",
       "      <td>-0.786354</td>\n",
       "      <td>0.092722</td>\n",
       "      <td>0.172740</td>\n",
       "      <td>0.939018</td>\n",
       "      <td>514.367072</td>\n",
       "      <td>258.919846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8090.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>754</td>\n",
       "      <td>5204.5977</td>\n",
       "      <td>2756.3560</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>-0.568414</td>\n",
       "      <td>-0.033909</td>\n",
       "      <td>0.157825</td>\n",
       "      <td>0.800631</td>\n",
       "      <td>691.066313</td>\n",
       "      <td>344.848645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>331</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>761</td>\n",
       "      <td>5704.5786</td>\n",
       "      <td>2961.0244</td>\n",
       "      <td>0.519061</td>\n",
       "      <td>-0.701369</td>\n",
       "      <td>0.099789</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>1.059587</td>\n",
       "      <td>474.249671</td>\n",
       "      <td>231.376864</td>\n",
       "      <td>0.997550</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9230.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>572</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>760</td>\n",
       "      <td>10132.8430</td>\n",
       "      <td>6128.6270</td>\n",
       "      <td>0.604828</td>\n",
       "      <td>-0.221403</td>\n",
       "      <td>-0.675996</td>\n",
       "      <td>0.171053</td>\n",
       "      <td>0.553850</td>\n",
       "      <td>1120.869565</td>\n",
       "      <td>638.839728</td>\n",
       "      <td>0.996584</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1014</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>743</td>\n",
       "      <td>6398.4560</td>\n",
       "      <td>3802.4944</td>\n",
       "      <td>0.594283</td>\n",
       "      <td>-0.231925</td>\n",
       "      <td>-0.632953</td>\n",
       "      <td>0.164199</td>\n",
       "      <td>0.543334</td>\n",
       "      <td>655.640646</td>\n",
       "      <td>365.431059</td>\n",
       "      <td>0.998150</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19370.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>892</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>749</td>\n",
       "      <td>5404.1533</td>\n",
       "      <td>3000.7166</td>\n",
       "      <td>0.555261</td>\n",
       "      <td>-0.424574</td>\n",
       "      <td>-0.241628</td>\n",
       "      <td>0.172230</td>\n",
       "      <td>1.237494</td>\n",
       "      <td>465.543391</td>\n",
       "      <td>239.973048</td>\n",
       "      <td>0.996873</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>970.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>785</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>765</td>\n",
       "      <td>7827.6973</td>\n",
       "      <td>3453.6255</td>\n",
       "      <td>0.441206</td>\n",
       "      <td>-0.537519</td>\n",
       "      <td>0.061153</td>\n",
       "      <td>0.048366</td>\n",
       "      <td>0.977242</td>\n",
       "      <td>666.751634</td>\n",
       "      <td>264.603756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>772</td>\n",
       "      <td>6629.6490</td>\n",
       "      <td>3733.9258</td>\n",
       "      <td>0.563216</td>\n",
       "      <td>-0.706442</td>\n",
       "      <td>-0.315641</td>\n",
       "      <td>0.202073</td>\n",
       "      <td>1.164527</td>\n",
       "      <td>779.966321</td>\n",
       "      <td>419.274426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>699</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>751</td>\n",
       "      <td>5549.1216</td>\n",
       "      <td>2844.7588</td>\n",
       "      <td>0.512650</td>\n",
       "      <td>-0.789140</td>\n",
       "      <td>0.082043</td>\n",
       "      <td>0.162450</td>\n",
       "      <td>1.230442</td>\n",
       "      <td>749.230360</td>\n",
       "      <td>359.185399</td>\n",
       "      <td>0.995955</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1044</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1110 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_rows  sales_mean  sales_std  sales_cv  sales_skew  sales_kurtosis  \\\n",
       "0          743   4593.7140  2435.7651  0.530239   -0.506928       -0.041492   \n",
       "1          741   5661.2210  2928.6194  0.517312   -0.786354        0.092722   \n",
       "2          754   5204.5977  2756.3560  0.529600   -0.568414       -0.033909   \n",
       "3          761   5704.5786  2961.0244  0.519061   -0.701369        0.099789   \n",
       "4          760  10132.8430  6128.6270  0.604828   -0.221403       -0.675996   \n",
       "...        ...         ...        ...       ...         ...             ...   \n",
       "1105       743   6398.4560  3802.4944  0.594283   -0.231925       -0.632953   \n",
       "1106       749   5404.1533  3000.7166  0.555261   -0.424574       -0.241628   \n",
       "1107       765   7827.6973  3453.6255  0.441206   -0.537519        0.061153   \n",
       "1108       772   6629.6490  3733.9258  0.563216   -0.706442       -0.315641   \n",
       "1109       751   5549.1216  2844.7588  0.512650   -0.789140        0.082043   \n",
       "\n",
       "      closed_fraction  weekend_ratio  customers_mean  customers_std  \\\n",
       "0            0.160162       0.829754      537.504711     265.109817   \n",
       "1            0.172740       0.939018      514.367072     258.919846   \n",
       "2            0.157825       0.800631      691.066313     344.848645   \n",
       "3            0.168200       1.059587      474.249671     231.376864   \n",
       "4            0.171053       0.553850     1120.869565     638.839728   \n",
       "...               ...            ...             ...            ...   \n",
       "1105         0.164199       0.543334      655.640646     365.431059   \n",
       "1106         0.172230       1.237494      465.543391     239.973048   \n",
       "1107         0.048366       0.977242      666.751634     264.603756   \n",
       "1108         0.202073       1.164527      779.966321     419.274426   \n",
       "1109         0.162450       1.230442      749.230360     359.185399   \n",
       "\n",
       "      pca_var_first  pca_components_90  store_type  assortment  \\\n",
       "0          0.997754                  1           0           0   \n",
       "1          0.000000                 18           0           2   \n",
       "2          0.000000                 18           0           2   \n",
       "3          0.997550                  1           3           2   \n",
       "4          0.996584                  1           0           2   \n",
       "...             ...                ...         ...         ...   \n",
       "1105       0.998150                  1           0           0   \n",
       "1106       0.996873                  1           3           2   \n",
       "1107       0.000000                 18           0           2   \n",
       "1108       0.000000                 18           0           0   \n",
       "1109       0.995955                  1           2           0   \n",
       "\n",
       "      competition_mean  competition_std  label  leaf_id  \n",
       "0                540.0              0.0     44        0  \n",
       "1               8090.0              0.0    346        1  \n",
       "2                670.0              0.0    331        2  \n",
       "3               9230.0              0.0    572        1  \n",
       "4                210.0              0.0   1014        3  \n",
       "...                ...              ...    ...      ...  \n",
       "1105           19370.0              0.0    892        4  \n",
       "1106             970.0              0.0    785        1  \n",
       "1107            2290.0              0.0    310        4  \n",
       "1108             180.0              0.0    699        6  \n",
       "1109             240.0              0.0   1044        6  \n",
       "\n",
       "[1110 rows x 18 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"label\"] = df_train[\"label\"].str.replace(\"store_\", \"\", regex=False).astype(int)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d4dc0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>CompetitionOpenSinceMonth</th>\n",
       "      <th>CompetitionOpenSinceYear</th>\n",
       "      <th>Promo2</th>\n",
       "      <th>Promo2SinceWeek</th>\n",
       "      <th>Promo2SinceYear</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>WeekOfYear</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>763</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>540.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>7076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346</td>\n",
       "      <td>2</td>\n",
       "      <td>663</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>8090.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>8129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>331</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>670.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>572</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d</td>\n",
       "      <td>c</td>\n",
       "      <td>9230.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1014</td>\n",
       "      <td>3</td>\n",
       "      <td>1234</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>210.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>12288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813759</th>\n",
       "      <td>932</td>\n",
       "      <td>6</td>\n",
       "      <td>535</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>15700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>4994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813760</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>1909</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>430.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>13145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813761</th>\n",
       "      <td>135</td>\n",
       "      <td>2</td>\n",
       "      <td>703</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "      <td>5190.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>9776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813762</th>\n",
       "      <td>923</td>\n",
       "      <td>4</td>\n",
       "      <td>609</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>280.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>4790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813763</th>\n",
       "      <td>249</td>\n",
       "      <td>6</td>\n",
       "      <td>487</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d</td>\n",
       "      <td>c</td>\n",
       "      <td>18010.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>4343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>813764 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  DayOfWeek  Customers  Open  Promo StateHoliday  SchoolHoliday  \\\n",
       "0          44          2        763     1      1            0              0   \n",
       "1         346          2        663     1      1            0              1   \n",
       "2         331          7          0     0      0            0              0   \n",
       "3         572          7          0     0      0            0              0   \n",
       "4        1014          3       1234     1      1            0              1   \n",
       "...       ...        ...        ...   ...    ...          ...            ...   \n",
       "813759    932          6        535     1      0            0              0   \n",
       "813760     25          2       1909     1      0            0              0   \n",
       "813761    135          2        703     1      1            0              1   \n",
       "813762    923          4        609     1      0            0              0   \n",
       "813763    249          6        487     1      0            0              0   \n",
       "\n",
       "       StoreType Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
       "0              a          a                540.0                        6.0   \n",
       "1              a          c               8090.0                        NaN   \n",
       "2              a          c                670.0                        NaN   \n",
       "3              d          c               9230.0                        4.0   \n",
       "4              a          c                210.0                        NaN   \n",
       "...          ...        ...                  ...                        ...   \n",
       "813759         a          a              15700.0                        NaN   \n",
       "813760         c          a                430.0                        4.0   \n",
       "813761         d          a               5190.0                        NaN   \n",
       "813762         a          a                280.0                        9.0   \n",
       "813763         d          c              18010.0                        9.0   \n",
       "\n",
       "        CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
       "0                         2011.0       0              NaN              NaN   \n",
       "1                            NaN       0              NaN              NaN   \n",
       "2                            NaN       1             14.0           2015.0   \n",
       "3                         2004.0       1             37.0           2009.0   \n",
       "4                            NaN       1             31.0           2013.0   \n",
       "...                          ...     ...              ...              ...   \n",
       "813759                       NaN       1             13.0           2010.0   \n",
       "813760                    2003.0       0              NaN              NaN   \n",
       "813761                       NaN       1              1.0           2013.0   \n",
       "813762                    2008.0       0              NaN              NaN   \n",
       "813763                    2014.0       0              NaN              NaN   \n",
       "\n",
       "        Year  Month  Day  WeekOfYear  Sales  \n",
       "0       2014      5    6          19   7076  \n",
       "1       2014      7   29          31   8129  \n",
       "2       2014      9   28          39      0  \n",
       "3       2013     11   17          46      0  \n",
       "4       2015      7   15          29  12288  \n",
       "...      ...    ...  ...         ...    ...  \n",
       "813759  2013     10    5          40   4994  \n",
       "813760  2013      5    7          19  13145  \n",
       "813761  2015      3   31          14   9776  \n",
       "813762  2015      3   12          11   4790  \n",
       "813763  2013      2    9           6   4343  \n",
       "\n",
       "[813764 rows x 20 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"main_train_df.csv\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary of all the raw data related to each store of each leaf\n",
    "raw_data_leaves = {}\n",
    "\n",
    "for leaf_id, leaf_df in df_train.groupby(\"leaf_id\"):\n",
    "    # Stores that belong to this leaf\n",
    "    store_ids = leaf_df[\"label\"].unique()\n",
    "\n",
    "    # Select raw data for these stores\n",
    "    raw_subset = raw_data[raw_data[\"Store\"].isin(store_ids)]\n",
    "\n",
    "    raw_data_leaves[leaf_id] = raw_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2ec2de4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>CompetitionOpenSinceMonth</th>\n",
       "      <th>CompetitionOpenSinceYear</th>\n",
       "      <th>Promo2</th>\n",
       "      <th>Promo2SinceWeek</th>\n",
       "      <th>Promo2SinceYear</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>WeekOfYear</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346</td>\n",
       "      <td>2</td>\n",
       "      <td>663</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>8090.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>8129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>572</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d</td>\n",
       "      <td>c</td>\n",
       "      <td>9230.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>201</td>\n",
       "      <td>3</td>\n",
       "      <td>488</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "      <td>20260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>6203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1111</td>\n",
       "      <td>1</td>\n",
       "      <td>746</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>27</td>\n",
       "      <td>11408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>918</td>\n",
       "      <td>4</td>\n",
       "      <td>568</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>18710.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>5159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813756</th>\n",
       "      <td>422</td>\n",
       "      <td>4</td>\n",
       "      <td>464</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>2880.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>4873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813757</th>\n",
       "      <td>457</td>\n",
       "      <td>2</td>\n",
       "      <td>426</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d</td>\n",
       "      <td>c</td>\n",
       "      <td>13140.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>4672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813761</th>\n",
       "      <td>135</td>\n",
       "      <td>2</td>\n",
       "      <td>703</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "      <td>5190.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>9776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813762</th>\n",
       "      <td>923</td>\n",
       "      <td>4</td>\n",
       "      <td>609</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>280.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>4790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813763</th>\n",
       "      <td>249</td>\n",
       "      <td>6</td>\n",
       "      <td>487</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d</td>\n",
       "      <td>c</td>\n",
       "      <td>18010.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>4343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354539 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  DayOfWeek  Customers  Open  Promo StateHoliday  SchoolHoliday  \\\n",
       "1         346          2        663     1      1            0              1   \n",
       "3         572          7          0     0      0            0              0   \n",
       "7         201          3        488     1      1            0              1   \n",
       "9        1111          1        746     1      1            0              0   \n",
       "10        918          4        568     1      0            0              0   \n",
       "...       ...        ...        ...   ...    ...          ...            ...   \n",
       "813756    422          4        464     1      0            0              0   \n",
       "813757    457          2        426     1      0            0              0   \n",
       "813761    135          2        703     1      1            0              1   \n",
       "813762    923          4        609     1      0            0              0   \n",
       "813763    249          6        487     1      0            0              0   \n",
       "\n",
       "       StoreType Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
       "1              a          c               8090.0                        NaN   \n",
       "3              d          c               9230.0                        4.0   \n",
       "7              d          a              20260.0                        NaN   \n",
       "9              a          a               1900.0                        6.0   \n",
       "10             a          c              18710.0                        4.0   \n",
       "...          ...        ...                  ...                        ...   \n",
       "813756         a          c               2880.0                        NaN   \n",
       "813757         d          c              13140.0                        NaN   \n",
       "813761         d          a               5190.0                        NaN   \n",
       "813762         a          a                280.0                        9.0   \n",
       "813763         d          c              18010.0                        9.0   \n",
       "\n",
       "        CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
       "1                            NaN       0              NaN              NaN   \n",
       "3                         2004.0       1             37.0           2009.0   \n",
       "7                            NaN       1             18.0           2014.0   \n",
       "9                         2014.0       1             31.0           2013.0   \n",
       "10                        2015.0       0              NaN              NaN   \n",
       "...                          ...     ...              ...              ...   \n",
       "813756                       NaN       0              NaN              NaN   \n",
       "813757                       NaN       1             31.0           2013.0   \n",
       "813761                       NaN       1              1.0           2013.0   \n",
       "813762                    2008.0       0              NaN              NaN   \n",
       "813763                    2014.0       0              NaN              NaN   \n",
       "\n",
       "        Year  Month  Day  WeekOfYear  Sales  \n",
       "1       2014      7   29          31   8129  \n",
       "3       2013     11   17          46      0  \n",
       "7       2013      7   31          31   6203  \n",
       "9       2014      6   30          27  11408  \n",
       "10      2013      7   11          28   5159  \n",
       "...      ...    ...  ...         ...    ...  \n",
       "813756  2014     12   11          50   4873  \n",
       "813757  2015      6    9          24   4672  \n",
       "813761  2015      3   31          14   9776  \n",
       "813762  2015      3   12          11   4790  \n",
       "813763  2013      2    9           6   4343  \n",
       "\n",
       "[354539 rows x 20 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_leaves[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7c8b89aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>leaf_id</th>\n",
       "      <th>n_rows</th>\n",
       "      <th>n_stores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>83044</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>354539</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>38320</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>30593</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>54704</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>73850</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>51058</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>62469</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>24793</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>14255</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>22518</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    leaf_id  n_rows  n_stores\n",
       "0         0   83044       114\n",
       "1         1  354539       492\n",
       "2         2   38320        51\n",
       "3         3   30593        41\n",
       "4         4   54704        74\n",
       "5         5   73850       100\n",
       "6         6   51058        69\n",
       "7         7   62469        86\n",
       "8         8   24793        34\n",
       "9         9   14255        19\n",
       "10       10   22518        30"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        \"leaf_id\": leaf_id,\n",
    "        \"n_rows\": len(df),\n",
    "        \"n_stores\": df[\"Store\"].nunique()\n",
    "    }\n",
    "    for leaf_id, df in raw_data_leaves.items()\n",
    "])\n",
    "\n",
    "summary.sort_values(\"leaf_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6863497f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"leaf_models_metadata/leaf_0\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       1.68 GB / 8.00 GB (21.0%)\n",
      "Disk Space Avail:   27.75 GB / 228.27 GB (12.2%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\t\tContext path: \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_0/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoGluon for leaf 0, samples = 83044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    LightGBMXT_BAG_L1    -303.731466 -318.058227  root_mean_squared_error       12.964498     231.752966   82.588748                12.964498              231.752966          82.588748            1       True          1\n",
      "1  WeightedEnsemble_L3    -303.731466 -318.058227  root_mean_squared_error       12.966466     231.753982   82.609278                 0.001968                0.001016           0.020530            3       True          4\n",
      "2  WeightedEnsemble_L2    -303.731466 -318.058227  root_mean_squared_error       12.967669     231.754448   82.596955                 0.003171                0.001482           0.008207            2       True          2\n",
      "3    LightGBMXT_BAG_L2    -306.997054 -334.851714  root_mean_squared_error       15.083092     248.557420  107.754395                 2.118594               16.804455          25.165647            2       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t170s\t = DyStack   runtime |\t430s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 430s\n",
      "AutoGluon will save models to \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_0\"\n",
      "Train Data Rows:    83044\n",
      "Train Data Columns: 19\n",
      "Label Column:       Sales\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2210.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 23.92 MB (1.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])    : 11 | ['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo', ...]\n",
      "\t\t('object', []) :  3 | ['StateHoliday', 'StoreType', 'Assortment']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 1 | ['StateHoliday']\n",
      "\t\t('float', [])     : 5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])       : 7 | ['Store', 'DayOfWeek', 'Customers', 'Year', 'Month', ...]\n",
      "\t\t('int', ['bool']) : 6 | ['Open', 'Promo', 'SchoolHoliday', 'StoreType', 'Assortment', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t19 features in original data used to generate 19 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.16 MB (0.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.27s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 286.54s of the 429.90s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.99%)\n",
      "\t-312.8059\t = Validation score   (-root_mean_squared_error)\n",
      "\t127.17s\t = Training   runtime\n",
      "\t206.7s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 144.65s of the 288.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.44%)\n",
      "\t-302.328\t = Validation score   (-root_mean_squared_error)\n",
      "\t82.34s\t = Training   runtime\n",
      "\t90.68s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 50.86s of the 194.22s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 166 due to low memory. Expected memory usage reduced from 26.98% -> 15.0% of available memory...\n",
      "\t-375.8282\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.91s\t = Training   runtime\n",
      "\t1.61s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 33.00s of the 176.37s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.26% memory usage per fold, 44.51%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=22.26%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=50137, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=50137, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 30.38s of the 173.74s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 163 due to low memory. Expected memory usage reduced from 27.54% -> 15.0% of available memory...\n",
      "2026-01-09 11:50:54,437\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=50136, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=50136, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-372.2682\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.2s\t = Training   runtime\n",
      "\t1.57s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 21.18s of the 164.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.68%)\n",
      "\t-534.3194\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.63s\t = Training   runtime\n",
      "\t1.65s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 0.72s of the 144.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.01%)\n",
      "\t-2797.9046\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.66s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 136.35s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.611, 'LightGBMXT_BAG_L1': 0.333, 'ExtraTreesMSE_BAG_L1': 0.056}\n",
      "\t-297.6996\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 136.17s of the 136.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.69%)\n",
      "\t-319.1519\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.24s\t = Training   runtime\n",
      "\t23.24s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 87.02s of the 86.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.37%)\n",
      "\t-305.3759\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.38s\t = Training   runtime\n",
      "\t1.78s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 74.79s of the 74.73s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 206 due to low memory. Expected memory usage reduced from 21.82% -> 15.0% of available memory...\n",
      "\tWarning: Reducing model 'n_estimators' from 206 -> 177 due to low time. Expected time usage reduced from 86.8s -> 74.7s...\n",
      "\t-307.3808\t = Validation score   (-root_mean_squared_error)\n",
      "\t51.9s\t = Training   runtime\n",
      "\t1.95s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 20.39s of the 20.33s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.62% memory usage per fold, 43.24%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=21.62%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=50268, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=50268, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 17.98s of the 17.92s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 153 due to low memory. Expected memory usage reduced from 29.39% -> 15.0% of available memory...\n",
      "2026-01-09 11:53:30,999\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=50267, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=50267, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-304.12\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.09s\t = Training   runtime\n",
      "\t1.57s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 5.00s of the 4.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.49%)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -5.20s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.48, 'LightGBMXT_BAG_L1': 0.28, 'ExtraTreesMSE_BAG_L2': 0.12, 'ExtraTreesMSE_BAG_L1': 0.04, 'LightGBM_BAG_L2': 0.04, 'RandomForestMSE_BAG_L2': 0.04}\n",
      "\t-297.4109\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 435.51s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 34.4 rows/s (10381 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_0\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       2.81 GB / 8.00 GB (35.2%)\n",
      "Disk Space Avail:   26.05 GB / 228.27 GB (11.4%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\t\tContext path: \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_1/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoGluon for leaf 1, samples = 354539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 11:53:54,056\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-01-09 11:53:54,057\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-01-09 11:53:54,058\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-01-09 11:53:54,058\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-01-09 11:53:54,060\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-01-09 11:53:54,061\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-01-09 11:53:54,062\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3    -337.864977 -342.028325  root_mean_squared_error       11.658265     151.429092  121.731445                 0.002225                0.007489           0.209565            3       True          4\n",
      "1    LightGBMXT_BAG_L2    -339.321246 -346.450985  root_mean_squared_error       11.656040     151.421603  121.521880                 3.274407               14.808229          41.599470            2       True          3\n",
      "2    LightGBMXT_BAG_L1    -340.191195 -343.874278  root_mean_squared_error        8.381633     136.613374   79.922410                 8.381633              136.613374          79.922410            1       True          1\n",
      "3  WeightedEnsemble_L2    -340.191195 -343.874278  root_mean_squared_error        8.383864     136.620430   79.932973                 0.002231                0.007056           0.010563            2       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t167s\t = DyStack   runtime |\t433s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 433s\n",
      "AutoGluon will save models to \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_1\"\n",
      "Train Data Rows:    354539\n",
      "Train Data Columns: 19\n",
      "Label Column:       Sales\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2513.00 MB\n",
      "\tTrain Data (Original)  Memory Usage: 102.11 MB (4.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])    : 11 | ['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo', ...]\n",
      "\t\t('object', []) :  3 | ['StateHoliday', 'StoreType', 'Assortment']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 2 | ['StateHoliday', 'StoreType']\n",
      "\t\t('float', [])     : 5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])       : 7 | ['Store', 'DayOfWeek', 'Customers', 'Year', 'Month', ...]\n",
      "\t\t('int', ['bool']) : 5 | ['Open', 'Promo', 'SchoolHoliday', 'Assortment', 'Promo2']\n",
      "\t0.8s = Fit runtime\n",
      "\t19 features in original data used to generate 19 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 34.83 MB (1.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.85s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 288.16s of the 432.34s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.67% memory usage per fold, 42.67%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=10.67%)\n",
      "\t-311.0142\t = Validation score   (-root_mean_squared_error)\n",
      "\t272.4s\t = Training   runtime\n",
      "\t285.94s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 121.28s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t-311.0142\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 121.23s of the 121.18s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.36% memory usage per fold, 49.43%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=12.36%)\n",
      "\t-319.2604\t = Validation score   (-root_mean_squared_error)\n",
      "\t110.28s\t = Training   runtime\n",
      "\t65.83s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -0.66s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.957, 'LightGBMXT_BAG_L2': 0.043}\n",
      "\t-310.9961\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 434.11s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 126.0 rows/s (44318 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_1\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       2.46 GB / 8.00 GB (30.8%)\n",
      "Disk Space Avail:   25.71 GB / 228.27 GB (11.3%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\t\tContext path: \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_2/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoGluon for leaf 2, samples = 38320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3    -316.092661 -342.063682  root_mean_squared_error        7.493605      76.030474  88.163705                 0.001761                0.000550           0.034596            3       True         11\n",
      "1          LightGBM_BAG_L2    -317.288597 -350.323890  root_mean_squared_error        7.169060      75.303412  83.484571                 0.086731                0.611553           2.857025            2       True          8\n",
      "2      WeightedEnsemble_L2    -317.913559 -342.899024  root_mean_squared_error        5.930279      72.297505  61.077967                 0.001256                0.001582           0.039777            2       True          6\n",
      "3     ExtraTreesMSE_BAG_L2    -319.849767 -349.125879  root_mean_squared_error        7.405113      75.418371  85.272084                 0.322784                0.726512           4.644538            2       True         10\n",
      "4          LightGBM_BAG_L1    -320.177087 -349.086620  root_mean_squared_error        1.856239      22.629529  19.665373                 1.856239               22.629529          19.665373            1       True          2\n",
      "5   RandomForestMSE_BAG_L2    -321.763199 -354.846100  root_mean_squared_error        7.337307      75.366964  93.062446                 0.254978                0.675105          12.434900            2       True          9\n",
      "6        LightGBMXT_BAG_L2    -326.941308 -369.654703  root_mean_squared_error        7.612215      79.317302  90.361774                 0.529886                4.625443           9.734228            2       True          7\n",
      "7        LightGBMXT_BAG_L1    -334.976202 -361.749542  root_mean_squared_error        4.072784      49.666394  41.372817                 4.072784               49.666394          41.372817            1       True          1\n",
      "8   RandomForestMSE_BAG_L1    -414.332270 -446.085276  root_mean_squared_error        0.276747       0.740306   6.270951                 0.276747                0.740306           6.270951            1       True          3\n",
      "9     ExtraTreesMSE_BAG_L1    -439.616197 -469.300290  root_mean_squared_error        0.221984       0.588638   1.981317                 0.221984                0.588638           1.981317            1       True          4\n",
      "10  NeuralNetFastAI_BAG_L1    -764.685833 -854.378618  root_mean_squared_error        0.654575       1.066991  11.337088                 0.654575                1.066991          11.337088            1       True          5\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t167s\t = DyStack   runtime |\t433s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 433s\n",
      "AutoGluon will save models to \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_2\"\n",
      "Train Data Rows:    38320\n",
      "Train Data Columns: 19\n",
      "Label Column:       Sales\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2658.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 11.04 MB (0.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['Assortment']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])    : 11 | ['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo', ...]\n",
      "\t\t('object', []) :  2 | ['StateHoliday', 'StoreType']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 1 | ['StateHoliday']\n",
      "\t\t('float', [])     : 5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])       : 7 | ['Store', 'DayOfWeek', 'Customers', 'Year', 'Month', ...]\n",
      "\t\t('int', ['bool']) : 5 | ['Open', 'Promo', 'SchoolHoliday', 'StoreType', 'Promo2']\n",
      "\t0.1s = Fit runtime\n",
      "\t18 features in original data used to generate 18 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.73 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 288.68s of the 433.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.23%)\n",
      "\t-355.4715\t = Validation score   (-root_mean_squared_error)\n",
      "\t45.12s\t = Training   runtime\n",
      "\t71.89s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 232.82s of the 377.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.17%)\n",
      "\t-342.2472\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.67s\t = Training   runtime\n",
      "\t25.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 205.21s of the 349.65s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 201 due to low memory. Expected memory usage reduced from 22.37% -> 15.0% of available memory...\n",
      "\t-435.8804\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.62s\t = Training   runtime\n",
      "\t1.0s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 196.32s of the 340.76s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.72% memory usage per fold, 70.90%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=17.72%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=50736, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=50736, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 193.97s of the 338.41s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 149 due to low memory. Expected memory usage reduced from 30.19% -> 15.0% of available memory...\n",
      "\t-457.7751\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.56s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 189.47s of the 333.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.03%)\n",
      "2026-01-09 12:08:17,337\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=50735, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=50735, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "2026-01-09 12:08:17,359\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=50737, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=50737, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-402.7783\t = Validation score   (-root_mean_squared_error)\n",
      "\t64.98s\t = Training   runtime\n",
      "\t0.89s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 122.00s of the 266.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.95%)\n",
      "\t-348.0516\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.55s\t = Training   runtime\n",
      "\t4.4s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 102.13s of the 246.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.89%)\n",
      "\t-431.0461\t = Validation score   (-root_mean_squared_error)\n",
      "\t84.3s\t = Training   runtime\n",
      "\t0.71s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 13.88s of the 158.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.23%)\n",
      "\t-346.711\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.65s\t = Training   runtime\n",
      "\t16.23s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 135.87s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.32, 'LightGBMXT_BAG_L1': 0.2, 'XGBoost_BAG_L1': 0.2, 'LightGBMLarge_BAG_L1': 0.16, 'NeuralNetFastAI_BAG_L1': 0.12}\n",
      "\t-332.1814\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 135.80s of the 135.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.07%)\n",
      "\t-354.899\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.7s\t = Training   runtime\n",
      "\t4.61s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 118.00s of the 117.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.49%)\n",
      "\t-339.1433\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.31s\t = Training   runtime\n",
      "\t0.62s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 111.53s of the 111.48s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 220 due to low memory. Expected memory usage reduced from 20.45% -> 15.0% of available memory...\n",
      "\t-338.8666\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.08s\t = Training   runtime\n",
      "\t1.17s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 81.86s of the 81.81s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.09% memory usage per fold, 42.18%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=21.09%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=50980, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=50980, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 79.97s of the 79.91s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 154 due to low memory. Expected memory usage reduced from 29.15% -> 15.0% of available memory...\n",
      "2026-01-09 12:12:36,091\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=50979, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=50979, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-337.5483\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.71s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 74.18s of the 74.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.47%)\n",
      "\t-333.2728\t = Validation score   (-root_mean_squared_error)\n",
      "\t59.17s\t = Training   runtime\n",
      "\t0.7s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 12.52s of the 12.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.86%)\n",
      "\t-339.5912\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.5s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 4.05s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.273, 'LightGBM_BAG_L1': 0.136, 'RandomForestMSE_BAG_L2': 0.136, 'LightGBMXT_BAG_L1': 0.091, 'XGBoost_BAG_L1': 0.091, 'ExtraTreesMSE_BAG_L2': 0.091, 'XGBoost_BAG_L2': 0.091, 'NeuralNetFastAI_BAG_L1': 0.045, 'LightGBMLarge_BAG_L1': 0.045}\n",
      "\t-331.1404\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 429.3s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 39.7 rows/s (4790 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_2\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       2.82 GB / 8.00 GB (35.2%)\n",
      "Disk Space Avail:   23.83 GB / 228.27 GB (10.4%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\t\tContext path: \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_3/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoGluon for leaf 3, samples = 30593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3    -409.256857 -449.422692  root_mean_squared_error        8.654821      44.969810  101.412855                 0.003117                0.001809           0.207180            3       True         13\n",
      "1      WeightedEnsemble_L2    -410.333074 -451.296659  root_mean_squared_error        7.444723      42.159084   73.170268                 0.001808                0.001437           0.022374            2       True          7\n",
      "2     ExtraTreesMSE_BAG_L2    -417.177291 -458.061469  root_mean_squared_error        8.222645      43.771513   84.898436                 0.246086                0.508981           3.744860            2       True         11\n",
      "3          LightGBM_BAG_L1    -420.668207 -465.843258  root_mean_squared_error        1.489768      12.569264   11.172600                 1.489768               12.569264          11.172600            1       True          2\n",
      "4          LightGBM_BAG_L2    -420.839546 -459.397121  root_mean_squared_error        8.050845      43.824519   83.665983                 0.074286                0.561987           2.512407            2       True          9\n",
      "5   RandomForestMSE_BAG_L2    -421.363398 -460.790616  root_mean_squared_error        8.331333      43.897033   94.948408                 0.354774                0.634501          13.794832            2       True         10\n",
      "6           XGBoost_BAG_L1    -433.318461 -476.781645  root_mean_squared_error        0.366565       1.638311    5.599192                 0.366565                1.638311           5.599192            1       True          6\n",
      "7        LightGBMXT_BAG_L2    -436.187023 -483.418132  root_mean_squared_error        8.383746      46.886222   89.812079                 0.407187                3.623690           8.658503            2       True          8\n",
      "8        LightGBMXT_BAG_L1    -436.715931 -483.280109  root_mean_squared_error        4.975588      27.271627   23.189667                 4.975588               27.271627          23.189667            1       True          1\n",
      "9   NeuralNetFastAI_BAG_L1    -469.839652 -511.076727  root_mean_squared_error        0.610994       0.678445   33.186434                 0.610994                0.678445          33.186434            1       True          5\n",
      "10  NeuralNetFastAI_BAG_L2    -487.915156 -570.481121  root_mean_squared_error        8.324002      44.091186   88.411663                 0.347443                0.828654           7.258087            2       True         12\n",
      "11    ExtraTreesMSE_BAG_L1    -542.356766 -566.749775  root_mean_squared_error        0.238837       0.479851    2.746979                 0.238837                0.479851           2.746979            1       True          4\n",
      "12  RandomForestMSE_BAG_L1    -548.250087 -572.547624  root_mean_squared_error        0.294807       0.625034    5.258704                 0.294807                0.625034           5.258704            1       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t168s\t = DyStack   runtime |\t432s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 432s\n",
      "AutoGluon will save models to \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_3\"\n",
      "Train Data Rows:    30593\n",
      "Train Data Columns: 19\n",
      "Label Column:       Sales\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2522.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.81 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])    : 11 | ['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo', ...]\n",
      "\t\t('object', []) :  3 | ['StateHoliday', 'StoreType', 'Assortment']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['StateHoliday', 'StoreType', 'Assortment']\n",
      "\t\t('float', [])     : 5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])       : 7 | ['Store', 'DayOfWeek', 'Customers', 'Year', 'Month', ...]\n",
      "\t\t('int', ['bool']) : 4 | ['Open', 'Promo', 'SchoolHoliday', 'Promo2']\n",
      "\t0.1s = Fit runtime\n",
      "\t19 features in original data used to generate 19 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3.01 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 288.14s of the 432.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.12%)\n",
      "\t-474.4706\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.37s\t = Training   runtime\n",
      "\t43.07s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 251.65s of the 395.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.01%)\n",
      "\t-453.7681\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.85s\t = Training   runtime\n",
      "\t13.7s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 233.72s of the 377.89s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 199 due to low memory. Expected memory usage reduced from 22.5% -> 15.0% of available memory...\n",
      "\t-563.9252\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.44s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 226.28s of the 370.45s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 19.32% memory usage per fold, 77.27%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=19.32%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=51305, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=51305, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 223.81s of the 367.98s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 148 due to low memory. Expected memory usage reduced from 30.26% -> 15.0% of available memory...\n",
      "\t-554.2678\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.05s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 219.95s of the 364.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.56%)\n",
      "2026-01-09 12:17:43,942\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=51304, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=51304, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "2026-01-09 12:17:43,943\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=51306, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=51306, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-490.4331\t = Validation score   (-root_mean_squared_error)\n",
      "\t55.35s\t = Training   runtime\n",
      "\t0.79s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 161.98s of the 306.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.10%)\n",
      "\t-466.7042\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.02s\t = Training   runtime\n",
      "\t2.47s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 143.17s of the 287.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.73%)\n",
      "\t-525.2253\t = Validation score   (-root_mean_squared_error)\n",
      "\t117.45s\t = Training   runtime\n",
      "\t0.95s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 22.85s of the 167.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.82%)\n",
      "\t-466.1827\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.69s\t = Training   runtime\n",
      "\t10.54s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 141.78s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.36, 'LightGBMXT_BAG_L1': 0.2, 'NeuralNetFastAI_BAG_L1': 0.2, 'XGBoost_BAG_L1': 0.16, 'NeuralNetTorch_BAG_L1': 0.04, 'LightGBMLarge_BAG_L1': 0.04}\n",
      "\t-440.258\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 141.74s of the 141.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.31%)\n",
      "\t-474.1144\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.11s\t = Training   runtime\n",
      "\t3.28s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 127.42s of the 127.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.29%)\n",
      "\t-449.1417\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.0s\t = Training   runtime\n",
      "\t0.55s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 121.65s of the 121.62s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 204 due to low memory. Expected memory usage reduced from 22.04% -> 15.0% of available memory...\n",
      "\t-448.6706\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.1s\t = Training   runtime\n",
      "\t0.83s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 100.36s of the 100.33s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.04% memory usage per fold, 40.07%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=20.04%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=51544, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=51544, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 98.36s of the 98.33s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 165 due to low memory. Expected memory usage reduced from 27.17% -> 15.0% of available memory...\n",
      "2026-01-09 12:22:13,700\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=51543, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=51543, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-445.8163\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.18s\t = Training   runtime\n",
      "\t0.66s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 93.24s of the 93.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.20%)\n",
      "\t-442.2659\t = Validation score   (-root_mean_squared_error)\n",
      "\t58.01s\t = Training   runtime\n",
      "\t1.07s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 32.59s of the 32.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.66%)\n",
      "\t-446.649\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.2s\t = Training   runtime\n",
      "\t0.59s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 23.32s of the 23.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.22%)\n",
      "\t-480.7361\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.8s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -3.12s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.24, 'ExtraTreesMSE_BAG_L2': 0.16, 'LightGBM_BAG_L1': 0.12, 'NeuralNetFastAI_BAG_L1': 0.12, 'RandomForestMSE_BAG_L2': 0.12, 'XGBoost_BAG_L2': 0.12, 'LightGBMXT_BAG_L1': 0.08, 'XGBoost_BAG_L1': 0.04}\n",
      "\t-437.9025\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 435.88s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 52.0 rows/s (3825 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_3\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       1.80 GB / 8.00 GB (22.5%)\n",
      "Disk Space Avail:   21.99 GB / 228.27 GB (9.6%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\t\tContext path: \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_4/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoGluon for leaf 4, samples = 54704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     WeightedEnsemble_L3    -370.074527 -347.157181  root_mean_squared_error        7.697029      73.185077   92.346683                 0.002517                0.001157           0.041077            3       True          9\n",
      "1     WeightedEnsemble_L2    -371.117549 -347.414201  root_mean_squared_error        7.449846      72.113898   82.910752                 0.002378                0.001415           0.028433            2       True          5\n",
      "2         LightGBM_BAG_L1    -371.583997 -353.698696  root_mean_squared_error        2.439841      22.759310   25.624219                 2.439841               22.759310          25.624219            1       True          2\n",
      "3         LightGBM_BAG_L2    -373.449022 -355.604618  root_mean_squared_error        7.586351      72.829287   86.276858                 0.138883                0.716804           3.394539            2       True          7\n",
      "4  RandomForestMSE_BAG_L2    -377.558243 -365.631464  root_mean_squared_error        7.555629      72.467116   88.911067                 0.108161                0.354633           6.028748            2       True          8\n",
      "5       LightGBMXT_BAG_L2    -382.963053 -366.697550  root_mean_squared_error        8.354335      79.799848  101.666424                 0.906867                7.687366          18.784105            2       True          6\n",
      "6       LightGBMXT_BAG_L1    -383.652024 -361.821206  root_mean_squared_error        4.564794      47.866146   46.381207                 4.564794               47.866146          46.381207            1       True          1\n",
      "7  RandomForestMSE_BAG_L1    -447.307996 -424.329787  root_mean_squared_error        0.322833       1.068797    8.972991                 0.322833                1.068797           8.972991            1       True          3\n",
      "8    ExtraTreesMSE_BAG_L1    -449.555221 -432.002328  root_mean_squared_error        0.120000       0.418229    1.903902                 0.120000                0.418229           1.903902            1       True          4\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t162s\t = DyStack   runtime |\t438s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 438s\n",
      "AutoGluon will save models to \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_4\"\n",
      "Train Data Rows:    54704\n",
      "Train Data Columns: 19\n",
      "Label Column:       Sales\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2356.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 15.76 MB (0.7% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])    : 11 | ['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo', ...]\n",
      "\t\t('object', []) :  3 | ['StateHoliday', 'StoreType', 'Assortment']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 1 | ['StateHoliday']\n",
      "\t\t('float', [])     : 5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])       : 7 | ['Store', 'DayOfWeek', 'Customers', 'Year', 'Month', ...]\n",
      "\t\t('int', ['bool']) : 6 | ['Open', 'Promo', 'SchoolHoliday', 'StoreType', 'Assortment', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t19 features in original data used to generate 19 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.37 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 291.89s of the 437.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.88%)\n",
      "\t-362.8466\t = Validation score   (-root_mean_squared_error)\n",
      "\t49.35s\t = Training   runtime\n",
      "\t64.49s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 232.64s of the 378.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.54%)\n",
      "\t-354.9574\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.45s\t = Training   runtime\n",
      "\t26.06s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 198.50s of the 344.55s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 200 due to low memory. Expected memory usage reduced from 22.48% -> 15.0% of available memory...\n",
      "\t-421.3963\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.43s\t = Training   runtime\n",
      "\t1.23s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 186.57s of the 332.62s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.30% memory usage per fold, 40.60%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=20.30%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=51833, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=51833, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 184.14s of the 330.19s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 150 due to low memory. Expected memory usage reduced from 29.85% -> 15.0% of available memory...\n",
      "\t-417.9163\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.63s\t = Training   runtime\n",
      "\t0.93s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 179.35s of the 325.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.00%)\n",
      "2026-01-09 12:28:25,739\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=51832, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=51832, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-379.4296\t = Validation score   (-root_mean_squared_error)\n",
      "\t91.46s\t = Training   runtime\n",
      "\t1.23s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 85.21s of the 231.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.51%)\n",
      "\t-360.9326\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.37s\t = Training   runtime\n",
      "\t5.82s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 58.55s of the 204.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.33%)\n",
      "\t-457.7314\t = Validation score   (-root_mean_squared_error)\n",
      "\t48.26s\t = Training   runtime\n",
      "\t0.93s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 6.51s of the 152.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.12%)\n",
      "\t-368.8533\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.95s\t = Training   runtime\n",
      "\t4.2s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 137.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.353, 'LightGBMXT_BAG_L1': 0.235, 'NeuralNetFastAI_BAG_L1': 0.235, 'XGBoost_BAG_L1': 0.176}\n",
      "\t-344.5136\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 137.04s of the 136.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.81%)\n",
      "\t-360.9475\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.12s\t = Training   runtime\n",
      "\t10.28s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 108.55s of the 108.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.14%)\n",
      "\t-351.0751\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.04s\t = Training   runtime\n",
      "\t1.55s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 99.60s of the 99.54s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 214 due to low memory. Expected memory usage reduced from 20.99% -> 15.0% of available memory...\n",
      "\t-353.3802\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.49s\t = Training   runtime\n",
      "\t1.49s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 56.31s of the 56.24s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.15% memory usage per fold, 40.29%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=20.15%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=52074, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=52074, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 54.48s of the 54.41s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 172 due to low memory. Expected memory usage reduced from 26.04% -> 15.0% of available memory...\n",
      "2026-01-09 12:33:01,563\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=52075, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=52075, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-348.3566\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.98s\t = Training   runtime\n",
      "\t1.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 45.08s of the 45.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.66%)\n",
      "\t-350.8116\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.85s\t = Training   runtime\n",
      "\t1.07s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 4.75s of the 4.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.66%)\n",
      "\t-352.8033\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.01s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -4.08s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L2': 0.24, 'LightGBM_BAG_L1': 0.2, 'LightGBMXT_BAG_L1': 0.16, 'NeuralNetFastAI_BAG_L1': 0.16, 'XGBoost_BAG_L1': 0.08, 'LightGBM_BAG_L2': 0.04, 'RandomForestMSE_BAG_L2': 0.04, 'NeuralNetFastAI_BAG_L2': 0.04, 'XGBoost_BAG_L2': 0.04}\n",
      "\t-343.5456\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 442.42s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 64.2 rows/s (6838 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_4\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       2.86 GB / 8.00 GB (35.8%)\n",
      "Disk Space Avail:   20.21 GB / 228.27 GB (8.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\t\tContext path: \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_5/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoGluon for leaf 5, samples = 73850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    LightGBMXT_BAG_L1    -385.826664 -403.848167  root_mean_squared_error       15.822081     298.210096   74.433121                15.822081              298.210096          74.433121            1       True          1\n",
      "1  WeightedEnsemble_L3    -385.826664 -403.848167  root_mean_squared_error       15.823706     298.211010   74.448658                 0.001625                0.000914           0.015537            3       True          4\n",
      "2  WeightedEnsemble_L2    -385.826664 -403.848167  root_mean_squared_error       15.824875     298.211088   74.437447                 0.002794                0.000992           0.004326            2       True          2\n",
      "3    LightGBMXT_BAG_L2    -394.094040 -424.721776  root_mean_squared_error       17.889397     312.118959  100.258300                 2.067316               13.908863          25.825179            2       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t171s\t = DyStack   runtime |\t429s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 429s\n",
      "AutoGluon will save models to \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_5\"\n",
      "Train Data Rows:    73850\n",
      "Train Data Columns: 19\n",
      "Label Column:       Sales\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2672.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.27 MB (0.8% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])    : 11 | ['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo', ...]\n",
      "\t\t('object', []) :  3 | ['StateHoliday', 'StoreType', 'Assortment']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['StateHoliday', 'StoreType', 'Assortment']\n",
      "\t\t('float', [])     : 5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])       : 7 | ['Store', 'DayOfWeek', 'Customers', 'Year', 'Month', ...]\n",
      "\t\t('int', ['bool']) : 4 | ['Open', 'Promo', 'SchoolHoliday', 'Promo2']\n",
      "\t0.2s = Fit runtime\n",
      "\t19 features in original data used to generate 19 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.26 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.23s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 285.57s of the 428.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.13%)\n",
      "\t-407.681\t = Validation score   (-root_mean_squared_error)\n",
      "\t150.45s\t = Training   runtime\n",
      "\t407.21s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 111.08s of the 253.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.70%)\n",
      "\t-394.2751\t = Validation score   (-root_mean_squared_error)\n",
      "\t92.76s\t = Training   runtime\n",
      "\t155.33s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 129.25s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.647, 'LightGBMXT_BAG_L1': 0.353}\n",
      "\t-388.5502\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 129.16s of the 129.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.17%)\n",
      "\t-406.4258\t = Validation score   (-root_mean_squared_error)\n",
      "\t53.22s\t = Training   runtime\n",
      "\t30.38s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 63.79s of the 63.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.79%)\n",
      "\t-395.8039\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.15s\t = Training   runtime\n",
      "\t1.5s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 56.24s of the 56.21s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 157 due to low memory. Expected memory usage reduced from 28.6% -> 15.0% of available memory...\n",
      "\t-404.3673\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.53s\t = Training   runtime\n",
      "\t1.5s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 30.75s of the 30.72s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.75% memory usage per fold, 49.50%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=24.75%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=52455, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=52455, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 28.25s of the 28.21s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 143 due to low memory. Expected memory usage reduced from 31.34% -> 15.0% of available memory...\n",
      "2026-01-09 12:43:32,879\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=52454, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=52454, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-400.0425\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.58s\t = Training   runtime\n",
      "\t1.31s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 20.02s of the 19.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.90%)\n",
      "\t-406.7991\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.36s\t = Training   runtime\n",
      "\t1.66s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -2.36s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.435, 'LightGBMXT_BAG_L1': 0.217, 'LightGBM_BAG_L2': 0.217, 'LightGBMXT_BAG_L2': 0.087, 'RandomForestMSE_BAG_L2': 0.043}\n",
      "\t-387.3146\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 431.24s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 15.5 rows/s (9232 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_5\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       1.62 GB / 8.00 GB (20.3%)\n",
      "Disk Space Avail:   18.16 GB / 228.27 GB (8.0%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\t\tContext path: \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_6/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoGluon for leaf 6, samples = 51058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     WeightedEnsemble_L3    -349.817305 -379.429277  root_mean_squared_error        9.911628      78.692785   90.723291                 0.002815                0.000686           0.045201            3       True          8\n",
      "1     WeightedEnsemble_L2    -350.202613 -380.167087  root_mean_squared_error        9.615922      77.615468   78.932209                 0.004033                0.000732           0.016861            2       True          4\n",
      "2         LightGBM_BAG_L1    -355.519209 -386.623358  root_mean_squared_error        3.506776      30.253178   26.776278                 3.506776               30.253178          26.776278            1       True          2\n",
      "3         LightGBM_BAG_L2    -362.739936 -392.145354  root_mean_squared_error        9.730506      78.182297   80.869804                 0.118617                0.567562           1.954456            2       True          6\n",
      "4  RandomForestMSE_BAG_L2    -363.509301 -395.837350  root_mean_squared_error        9.790196      78.124537   88.723634                 0.178307                0.509802           9.808286            2       True          7\n",
      "5       LightGBMXT_BAG_L1    -368.004397 -404.991251  root_mean_squared_error        5.934757      46.928921   48.078833                 5.934757               46.928921          48.078833            1       True          1\n",
      "6       LightGBMXT_BAG_L2    -377.938299 -410.362379  root_mean_squared_error       11.324968      85.418871  104.614781                 1.713079                7.804136          25.699433            2       True          5\n",
      "7  RandomForestMSE_BAG_L1    -441.980388 -461.100743  root_mean_squared_error        0.170356       0.432637    4.060237                 0.170356                0.432637           4.060237            1       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t164s\t = DyStack   runtime |\t436s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 436s\n",
      "AutoGluon will save models to \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_6\"\n",
      "Train Data Rows:    51058\n",
      "Train Data Columns: 19\n",
      "Label Column:       Sales\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1802.56 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.71 MB (0.8% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['Assortment']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])    : 11 | ['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo', ...]\n",
      "\t\t('object', []) :  2 | ['StateHoliday', 'StoreType']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 2 | ['StateHoliday', 'StoreType']\n",
      "\t\t('float', [])     : 5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])       : 7 | ['Store', 'DayOfWeek', 'Customers', 'Year', 'Month', ...]\n",
      "\t\t('int', ['bool']) : 4 | ['Open', 'Promo', 'SchoolHoliday', 'Promo2']\n",
      "\t0.2s = Fit runtime\n",
      "\t18 features in original data used to generate 18 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 4.97 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 290.80s of the 436.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.26%)\n",
      "\t-396.4084\t = Validation score   (-root_mean_squared_error)\n",
      "\t50.34s\t = Training   runtime\n",
      "\t58.76s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 233.13s of the 378.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.59%)\n",
      "\t-377.4091\t = Validation score   (-root_mean_squared_error)\n",
      "\t26.04s\t = Training   runtime\n",
      "\t29.77s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 200.79s of the 346.30s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 204 due to low memory. Expected memory usage reduced from 22.01% -> 15.0% of available memory...\n",
      "\t-446.6975\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.8s\t = Training   runtime\n",
      "\t1.2s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 188.33s of the 333.83s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.37% memory usage per fold, 40.74%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=20.37%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=52581, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=52581, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 185.99s of the 331.50s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 158 due to low memory. Expected memory usage reduced from 28.48% -> 15.0% of available memory...\n",
      "\t-450.9274\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.85s\t = Training   runtime\n",
      "\t0.92s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 180.88s of the 326.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.62%)\n",
      "2026-01-09 12:48:33,437\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=52582, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=52582, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-413.688\t = Validation score   (-root_mean_squared_error)\n",
      "\t88.71s\t = Training   runtime\n",
      "\t1.54s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 89.50s of the 235.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.58%)\n",
      "\t-387.3851\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.63s\t = Training   runtime\n",
      "\t4.97s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 59.42s of the 204.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.31%)\n",
      "\t-501.2092\t = Validation score   (-root_mean_squared_error)\n",
      "\t48.94s\t = Training   runtime\n",
      "\t1.08s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 7.57s of the 153.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.86%)\n",
      "\t-391.3428\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.58s\t = Training   runtime\n",
      "\t5.26s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 132.90s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.368, 'NeuralNetFastAI_BAG_L1': 0.211, 'XGBoost_BAG_L1': 0.211, 'LightGBMXT_BAG_L1': 0.158, 'LightGBMLarge_BAG_L1': 0.053}\n",
      "\t-366.1755\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 132.84s of the 132.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.96%)\n",
      "\t-402.8833\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.33s\t = Training   runtime\n",
      "\t5.64s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 111.98s of the 111.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.04%)\n",
      "\t-380.4966\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.75s\t = Training   runtime\n",
      "\t1.08s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 103.64s of the 103.61s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 219 due to low memory. Expected memory usage reduced from 20.51% -> 15.0% of available memory...\n",
      "\t-378.5466\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.87s\t = Training   runtime\n",
      "\t1.4s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 61.79s of the 61.76s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.26% memory usage per fold, 42.53%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=21.26%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=52794, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=52794, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 59.69s of the 59.66s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 155 due to low memory. Expected memory usage reduced from 29.02% -> 15.0% of available memory...\n",
      "2026-01-09 12:53:05,216\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=52795, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=52795, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-371.2756\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.27s\t = Training   runtime\n",
      "\t1.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 51.96s of the 51.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.52%)\n",
      "\t-372.9328\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.07s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 10.37s of the 10.33s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.71%)\n",
      "\t-379.0365\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.07s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 1.30s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesMSE_BAG_L2': 0.286, 'LightGBM_BAG_L1': 0.238, 'NeuralNetFastAI_BAG_L1': 0.143, 'XGBoost_BAG_L1': 0.143, 'LightGBMXT_BAG_L1': 0.095, 'NeuralNetFastAI_BAG_L2': 0.048, 'XGBoost_BAG_L2': 0.048}\n",
      "\t-365.162\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 435.27s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 61.9 rows/s (6383 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_6\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       2.77 GB / 8.00 GB (34.6%)\n",
      "Disk Space Avail:   16.39 GB / 228.27 GB (7.2%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\t\tContext path: \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_7/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoGluon for leaf 7, samples = 62469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     WeightedEnsemble_L3    -328.276189 -327.462021  root_mean_squared_error        9.501860     142.122755  100.983098                 0.002960                0.000933           0.035529            3       True          8\n",
      "1         LightGBM_BAG_L2    -328.961111 -335.366391  root_mean_squared_error        9.218057     141.247949   85.707407                 0.124470                0.791411           3.519169            2       True          5\n",
      "2     WeightedEnsemble_L2    -329.444310 -327.622733  root_mean_squared_error        9.095801     140.457330   82.203987                 0.002214                0.000793           0.015749            2       True          3\n",
      "3         LightGBM_BAG_L1    -330.221152 -333.624306  root_mean_squared_error        3.091759      40.797772   25.575732                 3.091759               40.797772          25.575732            1       True          2\n",
      "4    ExtraTreesMSE_BAG_L2    -333.875900 -343.092224  root_mean_squared_error        9.215168     140.859275   83.972436                 0.121581                0.402738           1.784198            2       True          7\n",
      "5  RandomForestMSE_BAG_L2    -333.966758 -339.788068  root_mean_squared_error        9.374430     141.330410   97.428400                 0.280843                0.873873          15.240162            2       True          6\n",
      "6       LightGBMXT_BAG_L2    -340.916754 -348.743803  root_mean_squared_error        9.986378     147.195396   94.079236                 0.892792                6.738859          11.890998            2       True          4\n",
      "7       LightGBMXT_BAG_L1    -343.524326 -340.747657  root_mean_squared_error        6.001828      99.658766   56.612506                 6.001828               99.658766          56.612506            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t163s\t = DyStack   runtime |\t437s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 437s\n",
      "AutoGluon will save models to \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_7\"\n",
      "Train Data Rows:    62469\n",
      "Train Data Columns: 19\n",
      "Label Column:       Sales\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2113.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 17.99 MB (0.9% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['StoreType']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])    : 11 | ['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo', ...]\n",
      "\t\t('object', []) :  2 | ['StateHoliday', 'Assortment']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 1 | ['StateHoliday']\n",
      "\t\t('float', [])     : 5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])       : 7 | ['Store', 'DayOfWeek', 'Customers', 'Year', 'Month', ...]\n",
      "\t\t('int', ['bool']) : 5 | ['Open', 'Promo', 'SchoolHoliday', 'Assortment', 'Promo2']\n",
      "\t0.2s = Fit runtime\n",
      "\t18 features in original data used to generate 18 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.08 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 291.36s of the 437.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.30%)\n",
      "\t-340.2428\t = Validation score   (-root_mean_squared_error)\n",
      "\t77.18s\t = Training   runtime\n",
      "\t129.25s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 200.73s of the 346.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.69%)\n",
      "\t-330.3905\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.49s\t = Training   runtime\n",
      "\t43.37s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 153.38s of the 299.16s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 171 due to low memory. Expected memory usage reduced from 26.25% -> 15.0% of available memory...\n",
      "\t-423.8496\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.04s\t = Training   runtime\n",
      "\t1.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 140.70s of the 286.48s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.14% memory usage per fold, 42.28%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=21.14%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=53054, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53054, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 138.10s of the 283.88s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 159 due to low memory. Expected memory usage reduced from 28.23% -> 15.0% of available memory...\n",
      "2026-01-09 12:59:20,287\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=53055, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53055, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-446.9394\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.33s\t = Training   runtime\n",
      "\t1.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 132.31s of the 278.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.61%)\n",
      "\t-361.3543\t = Validation score   (-root_mean_squared_error)\n",
      "\t91.02s\t = Training   runtime\n",
      "\t0.99s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 38.88s of the 184.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.42%)\n",
      "\t-338.432\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.15s\t = Training   runtime\n",
      "\t9.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 3.14s of the 148.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.57%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 138.63s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.381, 'LightGBMXT_BAG_L1': 0.238, 'NeuralNetFastAI_BAG_L1': 0.19, 'XGBoost_BAG_L1': 0.19}\n",
      "\t-321.6913\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 138.58s of the 138.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.14%)\n",
      "2026-01-09 13:01:44,714\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-01-09 13:01:44,735\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-01-09 13:01:44,738\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-01-09 13:01:44,741\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-01-09 13:01:44,745\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-01-09 13:01:44,769\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-01-09 13:01:44,805\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-341.6911\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.45s\t = Training   runtime\n",
      "\t9.65s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 114.15s of the 114.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.04%)\n",
      "\t-327.9408\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.88s\t = Training   runtime\n",
      "\t1.09s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 105.30s of the 105.27s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 206 due to low memory. Expected memory usage reduced from 21.77% -> 15.0% of available memory...\n",
      "\t-327.0279\t = Validation score   (-root_mean_squared_error)\n",
      "\t37.75s\t = Training   runtime\n",
      "\t1.52s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 65.64s of the 65.61s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.08% memory usage per fold, 44.17%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=22.08%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=53269, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53269, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 63.58s of the 63.55s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 162 due to low memory. Expected memory usage reduced from 27.65% -> 15.0% of available memory...\n",
      "2026-01-09 13:02:59,901\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=53270, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53270, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-325.8785\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.41s\t = Training   runtime\n",
      "\t1.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 54.67s of the 54.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.34%)\n",
      "\t-323.2403\t = Validation score   (-root_mean_squared_error)\n",
      "\t43.01s\t = Training   runtime\n",
      "\t1.25s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 8.95s of the 8.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.81%)\n",
      "\t-328.8487\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.18s\t = Training   runtime\n",
      "\t1.64s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -1.27s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.227, 'RandomForestMSE_BAG_L2': 0.182, 'LightGBM_BAG_L1': 0.136, 'ExtraTreesMSE_BAG_L2': 0.136, 'LightGBMXT_BAG_L1': 0.091, 'NeuralNetFastAI_BAG_L1': 0.091, 'XGBoost_BAG_L1': 0.045, 'LightGBM_BAG_L2': 0.045, 'XGBoost_BAG_L2': 0.045}\n",
      "\t-320.6615\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 438.7s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 41.7 rows/s (7809 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_7\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       2.79 GB / 8.00 GB (34.9%)\n",
      "Disk Space Avail:   14.65 GB / 228.27 GB (6.4%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\t\tContext path: \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_8/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoGluon for leaf 8, samples = 24793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3    -351.256826 -371.573880  root_mean_squared_error        5.036418      23.683197  100.981408                 0.005872                0.000840           0.085071            3       True         11\n",
      "1   RandomForestMSE_BAG_L2    -353.965890 -381.843976  root_mean_squared_error        4.674668      21.808533   91.082449                 0.740494                0.866410          14.284852            2       True         10\n",
      "2      WeightedEnsemble_L2    -354.459951 -373.825209  root_mean_squared_error        3.207437      19.585722   68.231682                 0.002229                0.000410           0.030270            2       True          7\n",
      "3          LightGBM_BAG_L2    -355.105122 -392.500168  root_mean_squared_error        4.290053      22.815948   86.611485                 0.355879                1.873825           9.813888            2       True          9\n",
      "4           XGBoost_BAG_L1    -361.624932 -409.835560  root_mean_squared_error        0.360982       1.097327    6.523811                 0.360982                1.097327           6.523811            1       True          6\n",
      "5          LightGBM_BAG_L1    -366.202818 -392.878931  root_mean_squared_error        0.827874       6.017735   10.126023                 0.827874                6.017735          10.126023            1       True          2\n",
      "6        LightGBMXT_BAG_L2    -368.634092 -415.330008  root_mean_squared_error        4.773737      25.013844   92.808539                 0.839563                4.071721          16.010942            2       True          8\n",
      "7        LightGBMXT_BAG_L1    -375.742639 -415.183225  root_mean_squared_error        1.305914      12.050336   12.949316                 1.305914               12.050336          12.949316            1       True          1\n",
      "8   NeuralNetFastAI_BAG_L1    -408.566919 -418.238152  root_mean_squared_error        0.710438       0.419914   38.602262                 0.710438                0.419914          38.602262            1       True          5\n",
      "9     ExtraTreesMSE_BAG_L1    -480.171951 -493.416557  root_mean_squared_error        0.257020       0.516284    2.696801                 0.257020                0.516284           2.696801            1       True          4\n",
      "10  RandomForestMSE_BAG_L1    -482.060852 -493.329916  root_mean_squared_error        0.471946       0.840527    5.899384                 0.471946                0.840527           5.899384            1       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t169s\t = DyStack   runtime |\t431s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 431s\n",
      "AutoGluon will save models to \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_8\"\n",
      "Train Data Rows:    24793\n",
      "Train Data Columns: 19\n",
      "Label Column:       Sales\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3269.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.14 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['StoreType']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])    : 11 | ['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo', ...]\n",
      "\t\t('object', []) :  2 | ['StateHoliday', 'Assortment']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 1 | ['StateHoliday']\n",
      "\t\t('float', [])     : 5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])       : 7 | ['Store', 'DayOfWeek', 'Customers', 'Year', 'Month', ...]\n",
      "\t\t('int', ['bool']) : 5 | ['Open', 'Promo', 'SchoolHoliday', 'Assortment', 'Promo2']\n",
      "\t0.1s = Fit runtime\n",
      "\t18 features in original data used to generate 18 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 286.98s of the 430.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.71%)\n",
      "\t-414.1074\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.46s\t = Training   runtime\n",
      "\t11.58s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 268.97s of the 412.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.79%)\n",
      "\t-388.9412\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.05s\t = Training   runtime\n",
      "\t10.87s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 250.67s of the 394.26s of remaining time.\n",
      "\t-483.1773\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.54s\t = Training   runtime\n",
      "\t0.94s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 242.81s of the 386.40s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.25% memory usage per fold, 73.02%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=18.25%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=53509, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53509, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 240.90s of the 384.49s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 186 due to low memory. Expected memory usage reduced from 24.19% -> 15.0% of available memory...\n",
      "\t-480.9703\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.63s\t = Training   runtime\n",
      "\t0.58s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 237.31s of the 380.90s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.19%)\n",
      "2026-01-09 13:07:41,683\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=53510, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53510, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "2026-01-09 13:07:41,688\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=53508, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53508, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "2026-01-09 13:07:41,688\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=53507, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53507, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-413.1578\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.83s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 192.87s of the 336.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.79%)\n",
      "\t-406.3868\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.8s\t = Training   runtime\n",
      "\t1.31s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 180.09s of the 323.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.56%)\n",
      "\t-466.9506\t = Validation score   (-root_mean_squared_error)\n",
      "\t146.38s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 30.06s of the 173.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.16%)\n",
      "\t-393.5334\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.57s\t = Training   runtime\n",
      "\t5.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 13.85s of the 157.44s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.63% memory usage per fold, 58.50%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=14.63%)\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=53706, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53706, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 11.22s of the 154.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.66%)\n",
      "2026-01-09 13:11:31,355\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=53708, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53708, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "2026-01-09 13:11:31,360\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=53709, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53709, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "2026-01-09 13:11:31,361\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-967.9996\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.11s\t = Training   runtime\n",
      "\t0.58s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 139.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.318, 'NeuralNetFastAI_BAG_L1': 0.318, 'LightGBMLarge_BAG_L1': 0.182, 'LightGBMXT_BAG_L1': 0.136, 'XGBoost_BAG_L1': 0.045}\n",
      "\t-368.8986\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 139.04s of the 138.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.77%)\n",
      "\t-408.0689\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.61s\t = Training   runtime\n",
      "\t4.5s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 121.58s of the 121.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.28%)\n",
      "\t-388.1732\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.52s\t = Training   runtime\n",
      "\t0.91s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 112.86s of the 112.77s of remaining time.\n",
      "\t-373.6051\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.1s\t = Training   runtime\n",
      "\t0.98s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 88.25s of the 88.16s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.40% memory usage per fold, 73.59%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=18.40%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=53812, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53812, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 85.86s of the 85.77s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 173 due to low memory. Expected memory usage reduced from 25.94% -> 15.0% of available memory...\n",
      "\t-375.0803\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.23s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 80.46s of the 80.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.79%)\n",
      "2026-01-09 13:12:40,532\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=53811, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53811, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "2026-01-09 13:12:40,533\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=53809, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53809, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "2026-01-09 13:12:40,534\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=53810, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=53810, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-372.8008\t = Validation score   (-root_mean_squared_error)\n",
      "\t46.26s\t = Training   runtime\n",
      "\t0.71s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 31.78s of the 31.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.72%)\n",
      "\t-383.5537\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.47s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 24.45s of the 24.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.07%)\n",
      "\t-432.6587\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.42s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -2.10s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_BAG_L2': 0.36, 'LightGBM_BAG_L1': 0.16, 'NeuralNetFastAI_BAG_L1': 0.16, 'NeuralNetFastAI_BAG_L2': 0.16, 'XGBoost_BAG_L2': 0.08, 'LightGBMXT_BAG_L1': 0.04, 'LightGBMLarge_BAG_L1': 0.04}\n",
      "\t-365.2543\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.06s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 434.14s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 97.8 rows/s (3100 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_8\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       1.70 GB / 8.00 GB (21.3%)\n",
      "Disk Space Avail:   12.67 GB / 228.27 GB (5.6%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\t\tContext path: \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_9/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoGluon for leaf 9, samples = 14255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L2    -305.025293 -317.308718  root_mean_squared_error        2.232792      10.088833  43.929411                 0.001847                0.006710           0.093078            2       True          8\n",
      "1      WeightedEnsemble_L3    -305.226292 -317.188559  root_mean_squared_error        3.288179      12.350785  88.988254                 0.003251                0.001239           0.105138            3       True         14\n",
      "2          LightGBM_BAG_L2    -306.599685 -335.471161  root_mean_squared_error        2.845433      11.701522  78.238784                 0.047978                0.296203           2.673915            2       True         10\n",
      "3     ExtraTreesMSE_BAG_L2    -311.173248 -325.751044  root_mean_squared_error        3.040047      11.877330  78.931824                 0.242592                0.472011           3.366955            2       True         12\n",
      "4           XGBoost_BAG_L1    -311.743248 -334.654879  root_mean_squared_error        0.208675       0.711281   3.685458                 0.208675                0.711281           3.685458            1       True          6\n",
      "5          LightGBM_BAG_L1    -312.244451 -330.324936  root_mean_squared_error        0.396835       2.866627   5.488743                 0.396835                2.866627           5.488743            1       True          2\n",
      "6   NeuralNetFastAI_BAG_L2    -317.523240 -341.674497  root_mean_squared_error        2.968135      11.867492  89.124306                 0.170679                0.462173          13.559437            2       True         13\n",
      "7   RandomForestMSE_BAG_L2    -318.209354 -327.829436  root_mean_squared_error        3.042336      11.877536  85.516161                 0.244881                0.472217           9.951292            2       True         11\n",
      "8        LightGBMXT_BAG_L2    -319.754395 -345.100896  root_mean_squared_error        2.981515      12.735919  79.741122                 0.184060                1.330601           4.176253            2       True          9\n",
      "9        LightGBMXT_BAG_L1    -323.901993 -337.121555  root_mean_squared_error        1.121953       6.028811   8.694222                 1.121953                6.028811           8.694222            1       True          1\n",
      "10  NeuralNetFastAI_BAG_L1    -339.234288 -364.909069  root_mean_squared_error        0.503482       0.475404  25.967910                 0.503482                0.475404          25.967910            1       True          5\n",
      "11   NeuralNetTorch_BAG_L1    -378.741459 -409.903644  root_mean_squared_error        0.106777       0.418983  26.633114                 0.106777                0.418983          26.633114            1       True          7\n",
      "12  RandomForestMSE_BAG_L1    -388.473934 -420.530287  root_mean_squared_error        0.234457       0.452489   3.395489                 0.234457                0.452489           3.395489            1       True          3\n",
      "13    ExtraTreesMSE_BAG_L1    -388.483121 -413.073479  root_mean_squared_error        0.225277       0.451724   1.699934                 0.225277                0.451724           1.699934            1       True          4\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t161s\t = DyStack   runtime |\t439s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 439s\n",
      "AutoGluon will save models to \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_9\"\n",
      "Train Data Rows:    14255\n",
      "Train Data Columns: 19\n",
      "Label Column:       Sales\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2524.50 MB\n",
      "\tTrain Data (Original)  Memory Usage: 4.11 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])    : 11 | ['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo', ...]\n",
      "\t\t('object', []) :  3 | ['StateHoliday', 'StoreType', 'Assortment']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 1 | ['StateHoliday']\n",
      "\t\t('float', [])     : 5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])       : 7 | ['Store', 'DayOfWeek', 'Customers', 'Year', 'Month', ...]\n",
      "\t\t('int', ['bool']) : 6 | ['Open', 'Promo', 'SchoolHoliday', 'StoreType', 'Assortment', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t19 features in original data used to generate 19 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.40 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 438.83s of the 438.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.65%)\n",
      "\t-335.665\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.7s\t = Training   runtime\n",
      "\t8.59s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 425.10s of the 425.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.62%)\n",
      "\t-326.918\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.77s\t = Training   runtime\n",
      "\t3.52s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 414.95s of the 414.93s of remaining time.\n",
      "\t-416.1024\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.92s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 410.29s of the 410.28s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.25% memory usage per fold, 69.02%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=17.25%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=54191, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=54191, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 407.90s of the 407.88s of remaining time.\n",
      "\t-405.8445\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.09s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 403.77s of the 403.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.81%)\n",
      "2026-01-09 13:17:22,451\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=54193, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=54193, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "2026-01-09 13:17:22,457\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=54190, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=54190, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-357.9223\t = Validation score   (-root_mean_squared_error)\n",
      "\t39.32s\t = Training   runtime\n",
      "\t0.83s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 360.30s of the 360.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.60%)\n",
      "\t-330.8576\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.24s\t = Training   runtime\n",
      "\t0.85s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 346.84s of the 346.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.43%)\n",
      "\t-382.0177\t = Validation score   (-root_mean_squared_error)\n",
      "\t133.04s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 209.85s of the 209.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.49%)\n",
      "\t-339.1134\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.03s\t = Training   runtime\n",
      "\t2.52s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 201.09s of the 201.07s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.08% memory usage per fold, 64.33%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=16.08%)\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=54452, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=54452, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 197.98s of the 197.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.38%)\n",
      "2026-01-09 13:20:52,143\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=54454, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=54454, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "2026-01-09 13:20:52,188\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=54453, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=54453, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "2026-01-09 13:20:52,189\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=54451, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=54451, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-354.6593\t = Validation score   (-root_mean_squared_error)\n",
      "\t161.48s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 33.02s of the 33.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.22%)\n",
      "\t-320.0695\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.68s\t = Training   runtime\n",
      "\t24.49s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the -1.66s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_r131_BAG_L1': 0.375, 'LightGBMXT_BAG_L1': 0.167, 'XGBoost_BAG_L1': 0.167, 'NeuralNetTorch_r79_BAG_L1': 0.125, 'LightGBM_BAG_L1': 0.083, 'NeuralNetFastAI_BAG_L1': 0.083}\n",
      "\t-311.5185\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 440.68s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 46.0 rows/s (1782 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_9\")\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.4.0: Fri Mar 15 00:19:22 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8112\n",
      "CPU Count:          8\n",
      "Memory Avail:       3.02 GB / 8.00 GB (37.7%)\n",
      "Disk Space Avail:   11.76 GB / 228.27 GB (5.2%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\t\tContext path: \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_10/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoGluon for leaf 10, samples = 22518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3    -547.988559 -541.449871  root_mean_squared_error        6.699146      45.597932  100.908371                 0.002166                0.001228           0.045088            3       True         12\n",
      "1      WeightedEnsemble_L2    -549.901976 -542.448856  root_mean_squared_error        5.298653      42.284604   69.301219                 0.002193                0.000725           0.044682            2       True          7\n",
      "2          LightGBM_BAG_L1    -554.522739 -557.414493  root_mean_squared_error        1.449346      10.072030   11.581484                 1.449346               10.072030          11.581484            1       True          2\n",
      "3     ExtraTreesMSE_BAG_L2    -555.356543 -559.569910  root_mean_squared_error        6.315224      44.294692   80.827331                 0.164149                0.364398           2.428403            2       True         11\n",
      "4          LightGBM_BAG_L2    -555.532059 -552.570763  root_mean_squared_error        6.209128      44.350384   81.026488                 0.058053                0.420091           2.627560            2       True          9\n",
      "5   RandomForestMSE_BAG_L2    -557.286594 -558.125027  root_mean_squared_error        6.638926      45.176613   98.235723                 0.487851                1.246319          19.836795            2       True         10\n",
      "6           XGBoost_BAG_L1    -566.725898 -567.109447  root_mean_squared_error        0.328635       1.919399    5.809623                 0.328635                1.919399           5.809623            1       True          6\n",
      "7        LightGBMXT_BAG_L2    -589.017755 -586.141640  root_mean_squared_error        6.537058      47.084658   85.513460                 0.385983                3.154365           7.114532            2       True          8\n",
      "8        LightGBMXT_BAG_L1    -632.985266 -627.587674  root_mean_squared_error        2.782314      29.745933   16.763028                 2.782314               29.745933          16.763028            1       True          1\n",
      "9   NeuralNetFastAI_BAG_L1    -634.940193 -648.330087  root_mean_squared_error        0.736165       0.546516   35.102401                 0.736165                0.546516          35.102401            1       True          5\n",
      "10    ExtraTreesMSE_BAG_L1    -723.663586 -692.148113  root_mean_squared_error        0.399702       0.821217    2.703023                 0.399702                0.821217           2.703023            1       True          4\n",
      "11  RandomForestMSE_BAG_L1    -764.273226 -734.122844  root_mean_squared_error        0.454913       0.825198    6.439369                 0.454913                0.825198           6.439369            1       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t160s\t = DyStack   runtime |\t440s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 440s\n",
      "AutoGluon will save models to \"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_10\"\n",
      "Train Data Rows:    22518\n",
      "Train Data Columns: 19\n",
      "Label Column:       Sales\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2326.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.49 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])    : 11 | ['Store', 'DayOfWeek', 'Customers', 'Open', 'Promo', ...]\n",
      "\t\t('object', []) :  3 | ['StateHoliday', 'StoreType', 'Assortment']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 3 | ['StateHoliday', 'StoreType', 'Assortment']\n",
      "\t\t('float', [])     : 5 | ['CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
      "\t\t('int', [])       : 7 | ['Store', 'DayOfWeek', 'Customers', 'Year', 'Month', ...]\n",
      "\t\t('int', ['bool']) : 4 | ['Open', 'Promo', 'SchoolHoliday', 'Promo2']\n",
      "\t0.1s = Fit runtime\n",
      "\t19 features in original data used to generate 19 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.21 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 293.45s of the 440.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.94%)\n",
      "\t-619.7061\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.43s\t = Training   runtime\n",
      "\t36.75s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 260.63s of the 407.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.86%)\n",
      "\t-548.9383\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.8s\t = Training   runtime\n",
      "\t17.99s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 241.28s of the 388.11s of remaining time.\n",
      "\t-726.4448\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.04s\t = Training   runtime\n",
      "\t0.91s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 233.07s of the 379.90s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.44% memory usage per fold, 73.77%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=18.44%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=54839, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=54839, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 230.39s of the 377.22s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 173 due to low memory. Expected memory usage reduced from 25.87% -> 15.0% of available memory...\n",
      "\t-694.1933\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.45s\t = Training   runtime\n",
      "\t0.54s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 227.22s of the 374.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.20%)\n",
      "2026-01-09 13:27:55,294\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=54840, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=54840, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-625.3428\t = Validation score   (-root_mean_squared_error)\n",
      "\t41.7s\t = Training   runtime\n",
      "\t0.72s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 182.63s of the 329.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.79%)\n",
      "\t-556.3832\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.07s\t = Training   runtime\n",
      "\t1.94s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 167.24s of the 314.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.49%)\n",
      "\t-692.29\t = Validation score   (-root_mean_squared_error)\n",
      "\t108.48s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 55.26s of the 202.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.61%)\n",
      "\t-559.2489\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.26s\t = Training   runtime\n",
      "\t8.61s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 34.43s of the 181.26s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.17% memory usage per fold, 56.69%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=2, gpus=0, memory=14.17%)\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=55016, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=55016, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 31.94s of the 178.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.60%)\n",
      "2026-01-09 13:31:13,881\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=55015, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=55015, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "2026-01-09 13:31:13,919\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=55014, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=55014, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "2026-01-09 13:31:13,941\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=55013, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=55013, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-836.1248\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.73s\t = Training   runtime\n",
      "\t0.83s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 146.29s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.36, 'XGBoost_BAG_L1': 0.24, 'NeuralNetFastAI_BAG_L1': 0.16, 'LightGBMLarge_BAG_L1': 0.16, 'LightGBMXT_BAG_L1': 0.08}\n",
      "\t-532.0403\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 146.08s of the 145.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.74%)\n",
      "\t-578.6138\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.56s\t = Training   runtime\n",
      "\t2.99s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 126.98s of the 126.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.99%)\n",
      "\t-542.5788\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.66s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 121.74s of the 121.65s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 219 due to low memory. Expected memory usage reduced from 20.48% -> 15.0% of available memory...\n",
      "\t-543.8915\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.06s\t = Training   runtime\n",
      "\t0.73s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 103.73s of the 103.64s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.73% memory usage per fold, 41.47%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=4, gpus=0, memory=20.73%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=55134, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=55134, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 101.86s of the 101.77s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 177 due to low memory. Expected memory usage reduced from 25.31% -> 15.0% of available memory...\n",
      "\t-542.0424\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.56s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 97.44s of the 97.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.61%)\n",
      "2026-01-09 13:32:31,138\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=55133, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/__init__.py\", line 1, in <module>\n",
      "    from .core import (\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/core.py\", line 45, in <module>\n",
      "    from .plot_helpers import save_plot_file, try_plot_offline, OfflineMetricVisualizer\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/catboost/plot_helpers.py\", line 5, in <module>\n",
      "    from . import _catboost\n",
      "  File \"_catboost.pyx\", line 1, in init _catboost\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::_ray_fit()\u001b[39m (pid=55133, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 446, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1068, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 124, in _fit\n",
      "    try_import_catboost()\n",
      "  File \"/opt/anaconda3/envs/ag/lib/python3.11/site-packages/autogluon/common/utils/try_import.py\", line 82, in try_import_catboost\n",
      "    raise ImportError(\n",
      "ImportError: Import catboost failed. Numpy version may be outdated, Please ensure numpy version >=1.17.0. If it is not, please try 'pip uninstall numpy -y; pip install numpy>=1.17.0' Detailed info: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n",
      "\t-536.6708\t = Validation score   (-root_mean_squared_error)\n",
      "\t44.69s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 50.06s of the 49.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.44%)\n",
      "\t-539.8912\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.78s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 40.80s of the 40.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.98%)\n",
      "\t-619.7174\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.41s\t = Training   runtime\n",
      "\t0.61s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 0.86s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.182, 'XGBoost_BAG_L1': 0.136, 'RandomForestMSE_BAG_L2': 0.136, 'NeuralNetFastAI_BAG_L2': 0.136, 'XGBoost_BAG_L2': 0.136, 'NeuralNetFastAI_BAG_L1': 0.091, 'LightGBMLarge_BAG_L1': 0.091, 'LightGBMXT_BAG_L1': 0.045, 'ExtraTreesMSE_BAG_L2': 0.045}\n",
      "\t-530.3166\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 439.77s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 41.0 rows/s (2815 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/sepideghorbanian/Documents/Semester_5/Research_Project/leaf_models_metadata/leaf_10\")\n"
     ]
    }
   ],
   "source": [
    "# train a model on each leaf for the raw data\n",
    "TARGET = \"Sales\"\n",
    "base_save_path = \"leaf_models_metadata\"\n",
    "os.makedirs(base_save_path, exist_ok=True)\n",
    "\n",
    "leaf_models = {}\n",
    "\n",
    "for leaf_id, raw_leaf_df in raw_data_leaves.items():\n",
    "    print(f\"Training AutoGluon for leaf {leaf_id}, samples = {len(raw_leaf_df)}\")\n",
    "\n",
    "    # Skip tiny leaves (VERY important)\n",
    "    if len(raw_leaf_df) < 500:\n",
    "        print(f\"Skipping leaf {leaf_id} (too few samples)\")\n",
    "        continue\n",
    "\n",
    "    leaf_folder = os.path.join(base_save_path, f\"leaf_{leaf_id}\")\n",
    "\n",
    "    predictor = TabularPredictor(\n",
    "        label=TARGET,\n",
    "        eval_metric=\"rmse\",\n",
    "        path=leaf_folder\n",
    "    ).fit(\n",
    "        raw_leaf_df.drop(columns=[\"leaf_id\"], errors=\"ignore\"),\n",
    "        presets=\"best_quality\",\n",
    "        time_limit=600\n",
    "    )\n",
    "\n",
    "    leaf_models[leaf_id] = predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0a321bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload the dictionary of all the leaf models\n",
    "\n",
    "leaf_models = {}\n",
    "base_save_path = \"leaf_models_metadata\"\n",
    "\n",
    "for leaf_id in range(11):\n",
    "    leaf_folder = os.path.join(base_save_path, f\"leaf_{leaf_id}\")\n",
    "    leaf_models[leaf_id] = TabularPredictor.load(leaf_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "49957d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <autogluon.tabular.predictor.predictor.TabularPredictor at 0x37fcb0710>,\n",
       " 1: <autogluon.tabular.predictor.predictor.TabularPredictor at 0x37fc6ff90>,\n",
       " 2: <autogluon.tabular.predictor.predictor.TabularPredictor at 0x37fc75b50>,\n",
       " 3: <autogluon.tabular.predictor.predictor.TabularPredictor at 0x37f96b310>,\n",
       " 4: <autogluon.tabular.predictor.predictor.TabularPredictor at 0x37f757190>,\n",
       " 5: <autogluon.tabular.predictor.predictor.TabularPredictor at 0x37f6fcf50>,\n",
       " 6: <autogluon.tabular.predictor.predictor.TabularPredictor at 0x37f9034d0>,\n",
       " 7: <autogluon.tabular.predictor.predictor.TabularPredictor at 0x37f91e8d0>,\n",
       " 8: <autogluon.tabular.predictor.predictor.TabularPredictor at 0x37fcd6f10>,\n",
       " 9: <autogluon.tabular.predictor.predictor.TabularPredictor at 0x37fcb52d0>,\n",
       " 10: <autogluon.tabular.predictor.predictor.TabularPredictor at 0x37f74ef50>}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "93388f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_rows</th>\n",
       "      <th>sales_mean</th>\n",
       "      <th>sales_std</th>\n",
       "      <th>sales_cv</th>\n",
       "      <th>sales_skew</th>\n",
       "      <th>sales_kurtosis</th>\n",
       "      <th>closed_fraction</th>\n",
       "      <th>weekend_ratio</th>\n",
       "      <th>customers_mean</th>\n",
       "      <th>customers_std</th>\n",
       "      <th>pca_var_first</th>\n",
       "      <th>pca_components_90</th>\n",
       "      <th>store_type</th>\n",
       "      <th>assortment</th>\n",
       "      <th>competition_mean</th>\n",
       "      <th>competition_std</th>\n",
       "      <th>label</th>\n",
       "      <th>leaf_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>743</td>\n",
       "      <td>4593.7140</td>\n",
       "      <td>2435.7651</td>\n",
       "      <td>0.530239</td>\n",
       "      <td>-0.506928</td>\n",
       "      <td>-0.041492</td>\n",
       "      <td>0.160162</td>\n",
       "      <td>0.829754</td>\n",
       "      <td>537.504711</td>\n",
       "      <td>265.109817</td>\n",
       "      <td>0.997754</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>741</td>\n",
       "      <td>5661.2210</td>\n",
       "      <td>2928.6194</td>\n",
       "      <td>0.517312</td>\n",
       "      <td>-0.786354</td>\n",
       "      <td>0.092722</td>\n",
       "      <td>0.172740</td>\n",
       "      <td>0.939018</td>\n",
       "      <td>514.367072</td>\n",
       "      <td>258.919846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8090.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>754</td>\n",
       "      <td>5204.5977</td>\n",
       "      <td>2756.3560</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>-0.568414</td>\n",
       "      <td>-0.033909</td>\n",
       "      <td>0.157825</td>\n",
       "      <td>0.800631</td>\n",
       "      <td>691.066313</td>\n",
       "      <td>344.848645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_331</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>761</td>\n",
       "      <td>5704.5786</td>\n",
       "      <td>2961.0244</td>\n",
       "      <td>0.519061</td>\n",
       "      <td>-0.701369</td>\n",
       "      <td>0.099789</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>1.059587</td>\n",
       "      <td>474.249671</td>\n",
       "      <td>231.376864</td>\n",
       "      <td>0.997550</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9230.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_572</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>760</td>\n",
       "      <td>10132.8430</td>\n",
       "      <td>6128.6270</td>\n",
       "      <td>0.604828</td>\n",
       "      <td>-0.221403</td>\n",
       "      <td>-0.675996</td>\n",
       "      <td>0.171053</td>\n",
       "      <td>0.553850</td>\n",
       "      <td>1120.869565</td>\n",
       "      <td>638.839728</td>\n",
       "      <td>0.996584</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_1014</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>743</td>\n",
       "      <td>5103.3135</td>\n",
       "      <td>2652.5212</td>\n",
       "      <td>0.519764</td>\n",
       "      <td>-0.792899</td>\n",
       "      <td>0.236228</td>\n",
       "      <td>0.170929</td>\n",
       "      <td>0.962292</td>\n",
       "      <td>546.318977</td>\n",
       "      <td>267.520106</td>\n",
       "      <td>0.996651</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>15720.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_588</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>739</td>\n",
       "      <td>3603.9182</td>\n",
       "      <td>2032.6263</td>\n",
       "      <td>0.564005</td>\n",
       "      <td>-0.292446</td>\n",
       "      <td>0.045020</td>\n",
       "      <td>0.162382</td>\n",
       "      <td>0.755790</td>\n",
       "      <td>437.604871</td>\n",
       "      <td>222.133675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>770</td>\n",
       "      <td>5511.5947</td>\n",
       "      <td>3165.9922</td>\n",
       "      <td>0.574424</td>\n",
       "      <td>-0.402313</td>\n",
       "      <td>-0.381137</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.772786</td>\n",
       "      <td>607.420779</td>\n",
       "      <td>327.400547</td>\n",
       "      <td>0.998231</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>751</td>\n",
       "      <td>7614.8384</td>\n",
       "      <td>3988.4440</td>\n",
       "      <td>0.523773</td>\n",
       "      <td>-0.681120</td>\n",
       "      <td>-0.194850</td>\n",
       "      <td>0.162450</td>\n",
       "      <td>0.844251</td>\n",
       "      <td>733.103862</td>\n",
       "      <td>365.779886</td>\n",
       "      <td>0.997582</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_683</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>618</td>\n",
       "      <td>4536.4990</td>\n",
       "      <td>2377.7388</td>\n",
       "      <td>0.524135</td>\n",
       "      <td>-0.908772</td>\n",
       "      <td>-0.013375</td>\n",
       "      <td>0.177994</td>\n",
       "      <td>1.362193</td>\n",
       "      <td>475.323625</td>\n",
       "      <td>237.610073</td>\n",
       "      <td>0.996975</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>store_317</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1115 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_rows  sales_mean  sales_std  sales_cv  sales_skew  sales_kurtosis  \\\n",
       "0          743   4593.7140  2435.7651  0.530239   -0.506928       -0.041492   \n",
       "1          741   5661.2210  2928.6194  0.517312   -0.786354        0.092722   \n",
       "2          754   5204.5977  2756.3560  0.529600   -0.568414       -0.033909   \n",
       "3          761   5704.5786  2961.0244  0.519061   -0.701369        0.099789   \n",
       "4          760  10132.8430  6128.6270  0.604828   -0.221403       -0.675996   \n",
       "...        ...         ...        ...       ...         ...             ...   \n",
       "1110       743   5103.3135  2652.5212  0.519764   -0.792899        0.236228   \n",
       "1111       739   3603.9182  2032.6263  0.564005   -0.292446        0.045020   \n",
       "1112       770   5511.5947  3165.9922  0.574424   -0.402313       -0.381137   \n",
       "1113       751   7614.8384  3988.4440  0.523773   -0.681120       -0.194850   \n",
       "1114       618   4536.4990  2377.7388  0.524135   -0.908772       -0.013375   \n",
       "\n",
       "      closed_fraction  weekend_ratio  customers_mean  customers_std  \\\n",
       "0            0.160162       0.829754      537.504711     265.109817   \n",
       "1            0.172740       0.939018      514.367072     258.919846   \n",
       "2            0.157825       0.800631      691.066313     344.848645   \n",
       "3            0.168200       1.059587      474.249671     231.376864   \n",
       "4            0.171053       0.553850     1120.869565     638.839728   \n",
       "...               ...            ...             ...            ...   \n",
       "1110         0.170929       0.962292      546.318977     267.520106   \n",
       "1111         0.162382       0.755790      437.604871     222.133675   \n",
       "1112         0.181818       0.772786      607.420779     327.400547   \n",
       "1113         0.162450       0.844251      733.103862     365.779886   \n",
       "1114         0.177994       1.362193      475.323625     237.610073   \n",
       "\n",
       "      pca_var_first  pca_components_90  store_type  assortment  \\\n",
       "0          0.997754                  1           0           0   \n",
       "1          0.000000                 18           0           2   \n",
       "2          0.000000                 18           0           2   \n",
       "3          0.997550                  1           3           2   \n",
       "4          0.996584                  1           0           2   \n",
       "...             ...                ...         ...         ...   \n",
       "1110       0.996651                  1           3           2   \n",
       "1111       0.000000                 18           0           0   \n",
       "1112       0.998231                  1           0           0   \n",
       "1113       0.997582                  1           0           0   \n",
       "1114       0.996975                  1           3           0   \n",
       "\n",
       "      competition_mean  competition_std       label  leaf_id  \n",
       "0                540.0              0.0    store_44        0  \n",
       "1               8090.0              0.0   store_346        1  \n",
       "2                670.0              0.0   store_331        2  \n",
       "3               9230.0              0.0   store_572        1  \n",
       "4                210.0              0.0  store_1014        3  \n",
       "...                ...              ...         ...      ...  \n",
       "1110           15720.0              0.0   store_588        7  \n",
       "1111            2320.0              0.0   store_486        1  \n",
       "1112            7240.0              0.0   store_917        0  \n",
       "1113            2850.0              0.0   store_683        6  \n",
       "1114            3140.0              0.0   store_317        1  \n",
       "\n",
       "[1115 rows x 18 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba0e65",
   "metadata": {},
   "source": [
    "## Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "657433de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metadata(df):\n",
    "    meta = {}\n",
    "\n",
    "    # 1. Dataset size\n",
    "  \n",
    "    meta[\"num_rows\"] = len(df)\n",
    "\n",
    "\n",
    "    # 2. Predicted Sales statistics\n",
    "\n",
    "    if \"PredictedSales\" in df:\n",
    "        sales = df[\"PredictedSales\"].replace(0, np.nan).dropna()\n",
    "\n",
    "        meta[\"sales_mean\"] = sales.mean()\n",
    "        meta[\"sales_std\"] = sales.std()\n",
    "        meta[\"sales_cv\"] = sales.std() / (sales.mean() + 1e-9)\n",
    "\n",
    "        # Skew and kurtosis tell you whether the store has spikes\n",
    "        meta[\"sales_skew\"] = sales.skew()\n",
    "        meta[\"sales_kurtosis\"] = sales.kurtosis()\n",
    "\n",
    "        # How often the store is closed\n",
    "        meta[\"closed_fraction\"] = (df[\"Open\"] == 0).mean() if \"Open\" in df else 0.0\n",
    "    else:\n",
    "        meta[\"sales_mean\"] = None\n",
    "        meta[\"sales_std\"] = None\n",
    "        meta[\"sales_cv\"] = None\n",
    "        meta[\"sales_skew\"] = None\n",
    "        meta[\"sales_kurtosis\"] = None\n",
    "        meta[\"closed_fraction\"] = None\n",
    "\n",
    "\n",
    "    # 3. Weekend vs weekday pattern\n",
    "\n",
    "    if \"Sales\" in df and \"DayOfWeek\" in df:\n",
    "        weekday_sales = df.groupby(\"DayOfWeek\")[\"Sales\"].mean()\n",
    "        meta[\"weekend_ratio\"] = weekday_sales.get(6, 0) / (weekday_sales.mean() + 1e-9)\n",
    "    else:\n",
    "        meta[\"weekend_ratio\"] = None\n",
    "\n",
    "\n",
    "\n",
    "    #5. Customers Mean and std\n",
    "    meta[\"customers_mean\"] = df[\"Customers\"].mean()\n",
    "    meta[\"customers_std\"] = df[\"Customers\"].std()\n",
    "\n",
    "\n",
    "\n",
    "    # 6. Intrinsic dimensionality (PCA)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    try:\n",
    "        scaled = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "        pca = PCA()\n",
    "        pca.fit(scaled)\n",
    "        meta[\"pca_var_first\"] = pca.explained_variance_ratio_[0]\n",
    "        meta[\"pca_components_90\"] = int(np.searchsorted(np.cumsum(pca.explained_variance_ratio_), 0.90) + 1)\n",
    "    except Exception:\n",
    "        meta[\"pca_var_first\"] = 0.0\n",
    "        meta[\"pca_components_90\"] = len(numeric_cols)\n",
    "\n",
    "    \n",
    "    # 7. Categorical store features (ordinal encoding)\n",
    "   \n",
    "    store_type_map = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3}\n",
    "    assortment_map = {\"a\": 0, \"b\": 1, \"c\": 2}\n",
    "\n",
    "    if \"StoreType\" in df:\n",
    "        meta[\"store_type\"] = store_type_map.get(df[\"StoreType\"].iloc[0], -1)\n",
    "    else:\n",
    "        meta[\"store_type\"] = -1\n",
    "\n",
    "    if \"Assortment\" in df:\n",
    "        meta[\"assortment\"] = assortment_map.get(df[\"Assortment\"].iloc[0], -1)\n",
    "    else:\n",
    "        meta[\"assortment\"] = -1\n",
    "\n",
    "\n",
    "\n",
    "    # 8. Competition distance stats\n",
    "    \n",
    "    if \"CompetitionDistance\" in df:\n",
    "        comp = df[\"CompetitionDistance\"].fillna(df[\"CompetitionDistance\"].median())\n",
    "        meta[\"competition_mean\"] = comp.mean()\n",
    "        meta[\"competition_std\"] = comp.std()\n",
    "    else:\n",
    "        meta[\"competition_mean\"] = None\n",
    "        meta[\"competition_std\"] = None\n",
    "\n",
    "    return meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e65dc2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def predict_for_dataset(df):\n",
    "    \"\"\"\n",
    "    Predict sales for a full dataset using metadata-based CART routing\n",
    "    \"\"\"\n",
    "\n",
    "    # 1️⃣ Compute metadata (dataset-level)\n",
    "    metadata = compute_metadata(df)\n",
    "    meta_df = pd.DataFrame([metadata])\n",
    "\n",
    "    # 2️⃣ Determine CART leaf\n",
    "    cart_array = meta_df[cart_features].to_numpy()\n",
    "    leaf_id = get_leaf_id(cart_tree, cart_array[0])\n",
    "\n",
    "    print(f\"Predicted leaf: {leaf_id}\")\n",
    "\n",
    "    # 3️⃣ Load leaf-specific predictor\n",
    "    leaf_path = os.path.join(\"leaf_models_metadata\", f\"leaf_{leaf_id}\")\n",
    "\n",
    "    if not os.path.exists(leaf_path):\n",
    "        raise ValueError(f\"No trained model found for leaf {leaf_id}\")\n",
    "\n",
    "    predictor = TabularPredictor.load(leaf_path)\n",
    "\n",
    "    # 4️⃣ Predict all rows\n",
    "    preds = predictor.predict(df)\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "251db830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>CompetitionOpenSinceMonth</th>\n",
       "      <th>CompetitionOpenSinceYear</th>\n",
       "      <th>Promo2</th>\n",
       "      <th>Promo2SinceWeek</th>\n",
       "      <th>Promo2SinceYear</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>WeekOfYear</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>448</td>\n",
       "      <td>5</td>\n",
       "      <td>698</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>7418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1113</td>\n",
       "      <td>2</td>\n",
       "      <td>606</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>9260.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>5258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>408</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>1560.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>410</td>\n",
       "      <td>1</td>\n",
       "      <td>1206</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>40.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>11920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>764</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>520.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>46</td>\n",
       "      <td>4371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203440</th>\n",
       "      <td>282</td>\n",
       "      <td>4</td>\n",
       "      <td>753</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>5525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203441</th>\n",
       "      <td>1111</td>\n",
       "      <td>2</td>\n",
       "      <td>559</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>48</td>\n",
       "      <td>6679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203442</th>\n",
       "      <td>531</td>\n",
       "      <td>2</td>\n",
       "      <td>680</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>4030.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>6042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203443</th>\n",
       "      <td>904</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d</td>\n",
       "      <td>c</td>\n",
       "      <td>570.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203444</th>\n",
       "      <td>993</td>\n",
       "      <td>4</td>\n",
       "      <td>764</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>d</td>\n",
       "      <td>c</td>\n",
       "      <td>3460.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>7626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203445 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  DayOfWeek  Customers  Open  Promo StateHoliday  SchoolHoliday  \\\n",
       "0         448          5        698     1      0            0              0   \n",
       "1        1113          2        606     1      0            0              1   \n",
       "2         408          7          0     0      0            0              0   \n",
       "3         410          1       1206     1      1            0              0   \n",
       "4         193          1        764     1      0            0              0   \n",
       "...       ...        ...        ...   ...    ...          ...            ...   \n",
       "203440    282          4        753     1      1            0              0   \n",
       "203441   1111          2        559     1      1            0              0   \n",
       "203442    531          2        680     1      1            0              0   \n",
       "203443    904          7          0     0      0            0              0   \n",
       "203444    993          4        764     1      1            a              0   \n",
       "\n",
       "       StoreType Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
       "0              a          c               3970.0                        9.0   \n",
       "1              a          c               9260.0                        NaN   \n",
       "2              c          a               1560.0                        NaN   \n",
       "3              c          a                 40.0                       11.0   \n",
       "4              a          a                520.0                        NaN   \n",
       "...          ...        ...                  ...                        ...   \n",
       "203440         a          a               1220.0                       12.0   \n",
       "203441         a          a               1900.0                        6.0   \n",
       "203442         a          c               4030.0                        NaN   \n",
       "203443         d          c                570.0                        7.0   \n",
       "203444         d          c               3460.0                       10.0   \n",
       "\n",
       "        CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
       "0                         2009.0       0              NaN              NaN   \n",
       "1                            NaN       0              NaN              NaN   \n",
       "2                            NaN       1             45.0           2009.0   \n",
       "3                         2011.0       1             22.0           2012.0   \n",
       "4                            NaN       0              NaN              NaN   \n",
       "...                          ...     ...              ...              ...   \n",
       "203440                    2010.0       0              NaN              NaN   \n",
       "203441                    2014.0       1             31.0           2013.0   \n",
       "203442                       NaN       0              NaN              NaN   \n",
       "203443                    2013.0       1             14.0           2011.0   \n",
       "203444                    2013.0       1             10.0           2014.0   \n",
       "\n",
       "        Year  Month  Day  WeekOfYear  Sales  \n",
       "0       2014      9   26          39   7418  \n",
       "1       2013      7    9          28   5258  \n",
       "2       2013      2   10           6      0  \n",
       "3       2014      2    3           6  11920  \n",
       "4       2013     11   11          46   4371  \n",
       "...      ...    ...  ...         ...    ...  \n",
       "203440  2013      8   15          33   5525  \n",
       "203441  2014     11   25          48   6679  \n",
       "203442  2015      1   27           5   6042  \n",
       "203443  2015      2   22           8      0  \n",
       "203444  2014      6   19          25   7626  \n",
       "\n",
       "[203445 rows x 20 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"main_test_df.csv\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "dc84ea27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>CompetitionOpenSinceMonth</th>\n",
       "      <th>CompetitionOpenSinceYear</th>\n",
       "      <th>Promo2</th>\n",
       "      <th>Promo2SinceWeek</th>\n",
       "      <th>Promo2SinceYear</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>WeekOfYear</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>448</td>\n",
       "      <td>5</td>\n",
       "      <td>698</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>7418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>448</td>\n",
       "      <td>2</td>\n",
       "      <td>857</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>9772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>448</td>\n",
       "      <td>6</td>\n",
       "      <td>341</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>4011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>448</td>\n",
       "      <td>3</td>\n",
       "      <td>724</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>7901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604</th>\n",
       "      <td>448</td>\n",
       "      <td>5</td>\n",
       "      <td>848</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200428</th>\n",
       "      <td>448</td>\n",
       "      <td>3</td>\n",
       "      <td>870</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>11059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201197</th>\n",
       "      <td>448</td>\n",
       "      <td>2</td>\n",
       "      <td>745</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "      <td>9075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201292</th>\n",
       "      <td>448</td>\n",
       "      <td>5</td>\n",
       "      <td>834</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>39</td>\n",
       "      <td>9070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201493</th>\n",
       "      <td>448</td>\n",
       "      <td>6</td>\n",
       "      <td>453</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>52</td>\n",
       "      <td>5159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202410</th>\n",
       "      <td>448</td>\n",
       "      <td>3</td>\n",
       "      <td>717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>7999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  DayOfWeek  Customers  Open  Promo StateHoliday  SchoolHoliday  \\\n",
       "0         448          5        698     1      0            0              0   \n",
       "889       448          2        857     1      1            0              0   \n",
       "2009      448          6        341     1      0            0              0   \n",
       "2743      448          3        724     1      0            0              0   \n",
       "3604      448          5        848     1      1            0              0   \n",
       "...       ...        ...        ...   ...    ...          ...            ...   \n",
       "200428    448          3        870     1      1            0              0   \n",
       "201197    448          2        745     1      1            0              0   \n",
       "201292    448          5        834     1      1            0              0   \n",
       "201493    448          6        453     1      0            0              0   \n",
       "202410    448          3        717     1      0            0              1   \n",
       "\n",
       "       StoreType Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
       "0              a          c               3970.0                        9.0   \n",
       "889            a          c               3970.0                        9.0   \n",
       "2009           a          c               3970.0                        9.0   \n",
       "2743           a          c               3970.0                        9.0   \n",
       "3604           a          c               3970.0                        9.0   \n",
       "...          ...        ...                  ...                        ...   \n",
       "200428         a          c               3970.0                        9.0   \n",
       "201197         a          c               3970.0                        9.0   \n",
       "201292         a          c               3970.0                        9.0   \n",
       "201493         a          c               3970.0                        9.0   \n",
       "202410         a          c               3970.0                        9.0   \n",
       "\n",
       "        CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
       "0                         2009.0       0              NaN              NaN   \n",
       "889                       2009.0       0              NaN              NaN   \n",
       "2009                      2009.0       0              NaN              NaN   \n",
       "2743                      2009.0       0              NaN              NaN   \n",
       "3604                      2009.0       0              NaN              NaN   \n",
       "...                          ...     ...              ...              ...   \n",
       "200428                    2009.0       0              NaN              NaN   \n",
       "201197                    2009.0       0              NaN              NaN   \n",
       "201292                    2009.0       0              NaN              NaN   \n",
       "201493                    2009.0       0              NaN              NaN   \n",
       "202410                    2009.0       0              NaN              NaN   \n",
       "\n",
       "        Year  Month  Day  WeekOfYear  Sales  \n",
       "0       2014      9   26          39   7418  \n",
       "889     2013      3   19          12   9772  \n",
       "2009    2015      1   24           4   4011  \n",
       "2743    2015      6   24          26   7901  \n",
       "3604    2013      1   11           2   9128  \n",
       "...      ...    ...  ...         ...    ...  \n",
       "200428  2014     11    5          45  11059  \n",
       "201197  2014      9   16          38   9075  \n",
       "201292  2013      9   27          39   9070  \n",
       "201493  2013     12   28          52   5159  \n",
       "202410  2013      7   24          30   7999  \n",
       "\n",
       "[191 rows x 20 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df_test[df_test[\"Store\"]== 448]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6573e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted leaf: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>...</th>\n",
       "      <th>CompetitionOpenSinceYear</th>\n",
       "      <th>Promo2</th>\n",
       "      <th>Promo2SinceWeek</th>\n",
       "      <th>Promo2SinceYear</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>WeekOfYear</th>\n",
       "      <th>Sales</th>\n",
       "      <th>predicted_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>448</td>\n",
       "      <td>5</td>\n",
       "      <td>698</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>7418</td>\n",
       "      <td>5724.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>448</td>\n",
       "      <td>2</td>\n",
       "      <td>857</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>9772</td>\n",
       "      <td>8125.454590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>448</td>\n",
       "      <td>6</td>\n",
       "      <td>341</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>4011</td>\n",
       "      <td>3125.678223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743</th>\n",
       "      <td>448</td>\n",
       "      <td>3</td>\n",
       "      <td>724</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>7901</td>\n",
       "      <td>6106.313477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604</th>\n",
       "      <td>448</td>\n",
       "      <td>5</td>\n",
       "      <td>848</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9128</td>\n",
       "      <td>7625.145020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200428</th>\n",
       "      <td>448</td>\n",
       "      <td>3</td>\n",
       "      <td>870</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>11059</td>\n",
       "      <td>8496.915039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201197</th>\n",
       "      <td>448</td>\n",
       "      <td>2</td>\n",
       "      <td>745</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "      <td>9075</td>\n",
       "      <td>6783.205078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201292</th>\n",
       "      <td>448</td>\n",
       "      <td>5</td>\n",
       "      <td>834</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>39</td>\n",
       "      <td>9070</td>\n",
       "      <td>7558.923340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201493</th>\n",
       "      <td>448</td>\n",
       "      <td>6</td>\n",
       "      <td>453</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>52</td>\n",
       "      <td>5159</td>\n",
       "      <td>4042.080078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202410</th>\n",
       "      <td>448</td>\n",
       "      <td>3</td>\n",
       "      <td>717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>7999</td>\n",
       "      <td>5855.603516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  DayOfWeek  Customers  Open  Promo StateHoliday  SchoolHoliday  \\\n",
       "0         448          5        698     1      0            0              0   \n",
       "889       448          2        857     1      1            0              0   \n",
       "2009      448          6        341     1      0            0              0   \n",
       "2743      448          3        724     1      0            0              0   \n",
       "3604      448          5        848     1      1            0              0   \n",
       "...       ...        ...        ...   ...    ...          ...            ...   \n",
       "200428    448          3        870     1      1            0              0   \n",
       "201197    448          2        745     1      1            0              0   \n",
       "201292    448          5        834     1      1            0              0   \n",
       "201493    448          6        453     1      0            0              0   \n",
       "202410    448          3        717     1      0            0              1   \n",
       "\n",
       "       StoreType Assortment  CompetitionDistance  ...  \\\n",
       "0              a          c               3970.0  ...   \n",
       "889            a          c               3970.0  ...   \n",
       "2009           a          c               3970.0  ...   \n",
       "2743           a          c               3970.0  ...   \n",
       "3604           a          c               3970.0  ...   \n",
       "...          ...        ...                  ...  ...   \n",
       "200428         a          c               3970.0  ...   \n",
       "201197         a          c               3970.0  ...   \n",
       "201292         a          c               3970.0  ...   \n",
       "201493         a          c               3970.0  ...   \n",
       "202410         a          c               3970.0  ...   \n",
       "\n",
       "        CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
       "0                         2009.0       0              NaN              NaN   \n",
       "889                       2009.0       0              NaN              NaN   \n",
       "2009                      2009.0       0              NaN              NaN   \n",
       "2743                      2009.0       0              NaN              NaN   \n",
       "3604                      2009.0       0              NaN              NaN   \n",
       "...                          ...     ...              ...              ...   \n",
       "200428                    2009.0       0              NaN              NaN   \n",
       "201197                    2009.0       0              NaN              NaN   \n",
       "201292                    2009.0       0              NaN              NaN   \n",
       "201493                    2009.0       0              NaN              NaN   \n",
       "202410                    2009.0       0              NaN              NaN   \n",
       "\n",
       "        Year  Month  Day  WeekOfYear  Sales  predicted_sales  \n",
       "0       2014      9   26          39   7418      5724.328125  \n",
       "889     2013      3   19          12   9772      8125.454590  \n",
       "2009    2015      1   24           4   4011      3125.678223  \n",
       "2743    2015      6   24          26   7901      6106.313477  \n",
       "3604    2013      1   11           2   9128      7625.145020  \n",
       "...      ...    ...  ...         ...    ...              ...  \n",
       "200428  2014     11    5          45  11059      8496.915039  \n",
       "201197  2014      9   16          38   9075      6783.205078  \n",
       "201292  2013      9   27          39   9070      7558.923340  \n",
       "201493  2013     12   28          52   5159      4042.080078  \n",
       "202410  2013      7   24          30   7999      5855.603516  \n",
       "\n",
       "[191 rows x 21 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CART algo\n",
    "df_new = temp.copy()\n",
    "preds_new = predict_for_dataset(df_new)\n",
    "df_new[\"predicted_sales\"] = preds_new\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cd93db36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 2577910.5\n",
      "mae: 1366.41259765625\n"
     ]
    }
   ],
   "source": [
    "y_true = df_new[\"Sales\"]\n",
    "y_pred = df_new[\"predicted_sales\"]\n",
    "rmse = mean_squared_error(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"rmse: {rmse}\")\n",
    "print(f\"mae: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ff692342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 443204.3125\n",
      "mae: 496.8134765625\n"
     ]
    }
   ],
   "source": [
    "# Global model \n",
    "global_predictor = TabularPredictor.load(f\"new_global/\")\n",
    "y_pred = global_predictor.predict(df_new)\n",
    "y_true = df_new[\"Sales\"]\n",
    "rmse = mean_squared_error(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true , y_pred)\n",
    "print(f\"rmse: {rmse}\")\n",
    "print(f\"mae: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06da2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
